{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<ol> <li>Switches</li> <li>Routers</li> <li>Network Models</li> <li>Upper Layers</li> <li>Network Tier-Based Architecture</li> <li>Hybrid-Cloud</li> <li>Ethernet Cables</li> <li>Computer Networks and the Internet</li> <li>Application Layer</li> <li>Transport layer</li> <li>[[Network Layer]]</li> <li>[[control plane]]</li> <li>IP addresses</li> <li>Subnet Mask</li> <li>Subnetting (GPT notes)</li> <li>[[Linux networking commands]]</li> </ol>"},{"location":"notes/Application%20Layer/","title":"Application Layer","text":""},{"location":"notes/Application%20Layer/#principles","title":"Principles","text":"<ul> <li>Core principle of network dev is writing application that run on different end systems and communicate to each other over the network</li> <li>Example<ul> <li>Web application -&gt; browser program running on the host's end and the web server program running in the web server host.</li> </ul> </li> <li>You need to write a program that runs on both end systems</li> </ul>"},{"location":"notes/Application%20Layer/#network-application-architectures","title":"Network Application Architectures","text":""},{"location":"notes/Application%20Layer/#client-server","title":"Client server","text":"<ul> <li>Always on host - server which services requests from other hosts - clients</li> <li>Clients don't interact with each other, they interact directly to the server</li> <li>For larger scale applications w/ large clients, big &amp; maintained servers are used - data centers.</li> <li>Servers have a fixed address and are always on, so clients can contact the server by sending packets to the address at all times.</li> </ul>"},{"location":"notes/Application%20Layer/#p2p","title":"P2P","text":"<ul> <li>No contact with any dedicated servers</li> <li>Clients are interconnected with each other and directly interact with each other</li> <li>Clients are controlled by the users themselves</li> <li>P2P is self-scalable</li> <li>For example, in a P2P file-sharing application, although each peer generates workload by requesting files, each peer also adds service capacity to the system by distributing files to other peers.</li> </ul> <p>![[Pasted image 20240920232159.png]]</p>"},{"location":"notes/Application%20Layer/#processes","title":"Processes","text":""},{"location":"notes/Application%20Layer/#client-server-process","title":"Client Server Process","text":"<ul> <li>In the context of a communication session between a pair of processes, the process that initiates the communication (that is, initially contacts the other process at the beginning of the session) is labeled as the client. The process that waits to be contacted to begin the session is the server.</li> </ul>"},{"location":"notes/Application%20Layer/#interface-between-process-and-computer-network","title":"Interface between Process and computer network","text":"<ul> <li>Any message sent from one process to another must go through the underlying network</li> <li>Messages are sent to and fro through a software interface called sockets</li> <li>Socket is the interface between application layer and transport layer, commonly also called as Application Programming Interface.</li> <li>Application developer has full control over the application layer side of the socket but little control of the transport layer side of the socket.</li> </ul>"},{"location":"notes/Application%20Layer/#addressing-processes","title":"Addressing Processes","text":"<ul> <li>In order for a process to send message to another process on another host, the receiving process needs to have an address.</li> <li>To identify receiving process, two things are needed<ol> <li>Address of the host (IP address)</li> <li>Identifier that specified the receiving process in the destination host. (destination port number).</li> </ol> </li> </ul>"},{"location":"notes/Application%20Layer/#transport-services-avl-to-applications","title":"Transport services (avl to applications)","text":"<ul> <li>Reliable data transfer<ul> <li>To support application there must be a protocol that can ensure data sent from one client is sent and received completely by the client on the other end.</li> <li>When a transport-layer protocol doesn\u2019t provide reliable data transfer, some of the data sent by the sending process may never arrive at the receiving process. This may be acceptable for loss-tolerant applications</li> </ul> </li> <li>Throughput<ul> <li>rate at which senders sends the bits to the receivers side</li> <li>networks have multiple devices in a bandwidth, making the throughput fluctuate.</li> <li>a service provided by the transport layer is to guarantee throughput of r bits/sec upon request of the host.</li> <li>if the throughput requirement is not met, the hosts may retry or giveup. applications with througput requirements are bandwidth sensitive applications (ex: multimedia apps)</li> <li>on the other hand, elastic applications, can adjust the to the amount of available bandwidith. (ex: email, file transfer, web transfers).</li> </ul> </li> <li>Timing</li> <li>Security</li> </ul>"},{"location":"notes/Application%20Layer/#transport-services-provided-by-internet","title":"Transport services (provided by internet)","text":"<ul> <li>TCP<ul> <li>Congestion control mechanism</li> <li>Connection Oriented</li> <li>Reliable data transfer</li> </ul> </li> <li>Enhanced TCP with SSL<ul> <li>end-to-end encryption</li> <li>enhancements are made in application layer</li> <li>ssl has its own socket API</li> </ul> </li> <li>UDP<ul> <li>Connectionless oriented</li> <li>No congestion protocol</li> <li>lightweight and minimal</li> </ul> </li> </ul>"},{"location":"notes/Application%20Layer/#application-layer-protocols","title":"Application layer protocols","text":"<p>Application layer protocols define how an applications processes run on different end systems.</p>"},{"location":"notes/Application%20Layer/#web-http","title":"Web &amp; HTTP","text":""},{"location":"notes/Application%20Layer/#http-overview","title":"HTTP Overview","text":"<ul> <li>HTTP (HyperText Transfer Protocol) is a protocol used for transferring data over the web. It defines how messages are formatted and transmitted.</li> <li>Two implementations of HTTP, server and client</li> <li>Web server -&gt; server and web browsers -&gt; client </li> <li>First TCP connection b/w client and server is established and then communication is done through their socket interface</li> <li>HTTP doesn't need to worry about the data being lost or anything, TCP handles it</li> <li>HTTP is a stateless protocol</li> </ul>"},{"location":"notes/Application%20Layer/#persistent-non-persistent","title":"Persistent &amp; Non Persistent","text":"<ul> <li>Persistent - All req and res sent in one TCP connection</li> <li>Non Persistent - New connection made for every req and res</li> </ul>"},{"location":"notes/Application%20Layer/#http-connections","title":"HTTP connections","text":"<p>Round trip time - Time taken for packet to travel from client to server and back to client. RTT includes packet-propagation delays, packet queuing delays in intermediate routers and switches, and packet-processing delays.</p>"},{"location":"notes/Application%20Layer/#http-w-non-persistent-connections","title":"HTTP w/ non-persistent connections","text":"<ul> <li>1 RTT - Send TCP initiation request sent, server ack sent</li> <li>1 RTT - HTTP req w/ client ack sent, html file sent</li> <li>New connection for each requested object.</li> <li>TCP buffers and variables must be kept in both client and server (to maintain TCP state), this can be a burden to both sides.</li> <li>![[Pasted image 20240915190330.png]]</li> </ul>"},{"location":"notes/Application%20Layer/#http-w-persistent-connection","title":"HTTP w/ persistent connection","text":"<ul> <li>1 RTT - Req sent, HTML sent back.</li> <li>TCP connection is made and left on until there isn't any communication/req for a while (this can be configured)</li> <li>multiple Web pages residing on the same server can be sent from the server to the same client over a single persistent TCP connection. These requests for objects can be made back-to-back, without waiting for replies to pending requests (pipelining)</li> <li>Recent HTTP 1.1 allows multiple server requests to be interleaved in the same connection. It has a priority mechanism which selects which request to respond to.</li> <li>![[Pasted image 20240915190249.png]]</li> <li>![[Pasted image 20240915190401.png]] </li> </ul> <p>RTT = 2 * propagation time Total = 2RTT + transmit time</p>"},{"location":"notes/Application%20Layer/#http-message-format","title":"HTTP Message format","text":"<p>There's two types of messages, requests and responses</p> <p>![[Pasted image 20240921003952.png]]</p>"},{"location":"notes/Application%20Layer/#request-message","title":"Request Message","text":"<pre><code>GET /somedir/index.html HTTP/1.1  #request line\n# following lines are header lines\nHost: www.somedir.com # specifies where the object recides\nConnection: close # close the tcp connection after sending this request\nUser-agent: Mozilla/5.0\nAccept-language: fr\n</code></pre> <p><code>HEAD</code> method is almost similar to <code>GET</code> but it only responds with an HTTP msg but leaves out the requested object.</p>"},{"location":"notes/Application%20Layer/#response-message","title":"Response Message","text":"<pre><code>HTTP/1.1 200 OK\nConnection: close\nData: ...\nServer: ...\nLast-Modified: ...\nContent-Length: ...\nContent-Type: ...\n</code></pre>"},{"location":"notes/Application%20Layer/#cookies","title":"Cookies","text":"<p>HTTP servers are stateless, cookies help provide some sort of state to HTTP servers. Cookies help us keep track of user information.</p> <p>Four comp. of the cookie tech 1. Cookie header line in HTTP req msg 2. Cookie header line in HTTP res msg 3. Cookie file stored in users end system and managed by the browser 4. Backend database at the website.</p> <p>Coookies create a user session layer on top of the stateless HTTP. When visiting a site for the first time, the server will respond with a msg to browser with a msg,</p> <p><code>Set Cookie: 1679</code></p> <p>When revisiting, the browser sends requests with the cookie file,</p> <p><code>Cookie: 1679</code></p> <p>![[Pasted image 20240824102952.png]]</p>"},{"location":"notes/Application%20Layer/#web-caching","title":"Web Caching","text":"<ul> <li>aka Proxy server</li> <li>satisfies the webs http req on behalf of the web server</li> <li>has its own disk storage space &amp; keeps copies of recently requested objects</li> <li>both client &amp; server at the same time</li> <li>Usually purchased and installed by the ISP's</li> <li>ISP's purchase and install these servers in their network and configure their browsers to be directed to the proxy servers.</li> </ul> <p>Pros of web caching 1. Reduces traffic on main web server 2. Reduce response time 3. Improve performance of applications</p> <p>![[Pasted image 20240824145556.png]]</p>"},{"location":"notes/Application%20Layer/#content-distribution-networks","title":"Content Distribution Networks","text":"<p>Is another form of web caching where the ISP needs to only purchase a specific domain and need not worry about setting up an actual server.</p> <p>A CDN company installs many caches throughout the internet. there are shared CDNS &amp; dedicated CDNS.</p>"},{"location":"notes/Application%20Layer/#conditional-get","title":"Conditional GET","text":"<p>This is another form of an HTTP request which is required to check whether the existing object in the web cache is of the latest version or not.</p> <p>Once the web cache is stored, and it's accessed some time later, the web cache server will send a request to the web server as such</p> <pre><code>GET /fruit/kiwi.gif HTTP/1.1 \nHost: www.exotiquecuisine.com \nIf-modified-since: Wed, 9 Sep 2015 09:23:24\n</code></pre> <p>The <code>If-modified-since</code> is compared with the <code>Last-modified</code> header to come across any updates to the object.</p> <p>If it's not modified, the server will send this back to the proxy server</p> <pre><code>HTTP/1.1 304 Not Modified \nDate: Sat, 10 Oct 2015 15:39:29 \nServer: Apache/1.3.0 (Unix)\n</code></pre> <p>And if the object is modified,</p> <pre><code>HTTP/1.1 200 OK \nDate: Sat, 10 Oct 2015 15:39:29 \nServer: Apache/1.3.0 (Unix) \nLast-Modified: Sat, 10 Oct 2015 10:15:00 \nContent-Type: image/gif \nContent-Length: 1024\n</code></pre>"},{"location":"notes/Application%20Layer/#email-and-the-internet","title":"Email and the internet","text":"<p>Terminologies - User agents - Sends the mail to the mail server - Mail Servers - Stores the mail in its Queue and sends the mail to the other mail servers when needed. - Simple Mail Transfer Protocol (SMTP) - Application Layer protocol which is supported by TCP for reliable data transfer</p> <p>Flow of mail</p> <pre><code>(SMTP Client) Senders user agent -&gt; Senders Mail server -&gt; (SMTP Server) Recipient's Mail Server -&gt; Recipients Mail Box\n</code></pre> <p>![[Pasted image 20240825120924.png]]</p>"},{"location":"notes/Application%20Layer/#smtp","title":"SMTP","text":"<p>Facts - Much older than HTTP - Data is sent to &amp; fro in 7 bit ASCII format. While sending it's encoded by client and while receiving its decoded by server. - SMTP first initializes TCP connection with other mail server. - No intermediate mail servers are used for long distances.</p> <p>Process - Client SMTP has TCP establish a connection (port 25) with the Server SMTP - Client and server perform three-way handshaking where the client also mentions the senders and recipients email address. - The reliable data transfer is left off to TCP. - Process is repeated if there are more messages over the same connection or the connection is closed.</p> <p>![[Pasted image 20240825120936.png]]</p> <p>Example communication</p> <pre><code>S:\u00a0\u00a0220 hamburger.edu \nC:\u00a0\u00a0HELO crepes.fr # HELO, MAIL, RCPT TO, DATA, QUIT are part of the dialogue\nS:\u00a0\u00a0250 Hello crepes.fr, pleased to meet you \nC:\u00a0\u00a0MAIL FROM: &lt;alice@crepes.fr&gt;\nS:\u00a0\u00a0250 alice@crepes.fr ... Sender ok \nC:\u00a0\u00a0RCPT TO: &lt;bob@hamburger.edu&gt;\nS:\u00a0\u00a0250 bob@hamburger.edu ... Recipient ok \nC:\u00a0\u00a0DATA \nS:\u00a0\u00a0354 Enter mail, end with \u201d.\u201d on a line by itself \nC:\u00a0\u00a0Do you like ketchup? \nC:\u00a0\u00a0How about pickles? \nC:\u00a0\u00a0. # end of message, in ASCII -&gt; CRLF (carriage return &amp; line feed)\nS:\u00a0\u00a0250 Message accepted for delivery \nC:\u00a0\u00a0QUIT \nS:\u00a0\u00a0221 hamburger.edu closing connection\n</code></pre>"},{"location":"notes/Application%20Layer/#comparison-w-http","title":"Comparison w/ HTTP","text":"HTTP SMTP Pull protocol, where the users use TCP connection to pull the info from the server Push Protocol, where the sender initiates a TCP connection to send data Doesn't need any modification to the data being transferred Data transferred must be encoded/decoded in 7-Bit ASCII format Encapsulates each object in its own HTTP msg Places all message's objects in one object ### Mail Access formats <p>Flow - Senders user agent sends the msg to its mail server. (using smtp) - Senders mail server sends msg to recipients mail server. (using smtp) - Since SMTP is a push protocol, the recipients user agent doesn't have any way to get the msg from its mail server. - Mail access protocol helps user agents pull the msg from their mail server.</p> <p>Mail access protocols - IMAP - POP3 - HTTP</p> <ol> <li>POP3<ul> <li>POP3 begins when the client makes a TCP connection to the mail server on port 110.</li> <li>Three phases in POP3<ol> <li>Authorization - user is authorized </li> <li>Transaction - user agent retrieves messages, mark msg for deletion, remove deletion marks and obtain mail statistics</li> <li>Update - occurs after <code>quit</code> command is run</li> </ol> </li> <li>Server responds with <code>+OK</code> &amp; <code>-ERR</code></li> <li>Authorization has two principle commands, <code>user&lt;username&gt;</code> &amp; <code>pass&lt;password&gt;</code> </li> <li>Transaction phase has two methods which can be configured by the user, \"download and delete\" &amp; \"download and keep\".<ul> <li>download and delete mode allows the user to use the <code>list, retr, dele</code> commands</li> <li>in download and keep mode the user agent leaves the messages on the mail server after downloading them.</li> </ul> </li> <li>After the user issues wtv commands they want, the user runs <code>quit</code> and POP3 server enters the update phase</li> <li>This mode isn't really useful if the user is a nomad who wants to check their mails on different servers over different laptops.</li> <li>POP3 does not carry state information over sessions, it only keeps state information (i.e which mails have been marked for deletion) within a session.</li> </ul> </li> <li>IMAP<ul> <li>With POP3 one can download the mails on their local machine and maintain different folder for the mails. But this paradigm doesn't allow the user to use these mails on different machines.</li> <li>IMAP is a mail access protocol that allows the user to move mails into different folder, create new folder and move msg's from one folder to other.</li> <li>IMAP maintains state information across sessions</li> <li>Allows user to access/download only a part of a message in low bandwidth connection areas.</li> </ul> </li> </ol> <p>In web-based emails, the servers and agents now connect over HTTP. The user agent pulls emails from the mail server using HTTP rather than POP3 and IMAP. When the user wants to send an email, the user agent sends it to the mail server using HTTP. However the mail servers still send and receive messages through SMTP.</p> Feature POP3 IMAP Email Storage Downloads emails to local device; deletes from server. Emails remain on the server; only synchronized. Device Access Accessed from one device at a time. Accessible from multiple devices simultaneously. Email Organization Limited organization; cannot create folders on the server. Allows creation and management of folders on the server. Synchronization No synchronization; changes not reflected across devices. Synchronizes changes across all devices (e.g., read/unread status). Offline Access Emails can be read offline after download. Requires internet connection to access emails fully. Connection Ports Port 110 (unencrypted), 995 (encrypted). Port 143 (unencrypted), 993 (encrypted). Use Case Suitable for single-device use, limited internet access. Ideal for users needing access from multiple devices. Server Load Lower server load due to local storage of emails. Higher server load due to email storage and synchronization. Backup and Recovery Emails lost if deleted after download unless kept on server. Multiple copies stored, allowing recovery if local data is lost. Complexity Simpler and easier to implement. More complex, requiring more resources and management."},{"location":"notes/Application%20Layer/#dns-servers","title":"DNS Servers","text":"<ul> <li>Two ways to identify a host, hostname &amp; ip address. </li> <li>DNS servers help in resolving the hostnames into ip addresses so that routers/switches can operate.</li> <li>Operates on port 53 and runs over UDP</li> <li>Deployed by application layer protocols (HTTP &amp; SMTP) to resolve hostnames.</li> <li>Adds additional delay, but sometimes IP addresses are cached in nearby servers.</li> </ul> <p>DNS is a  1. Distributed database implemented in a hierarchy of DNS servers 2. Allows hosts to query the distributed database.</p> <p>Features 1. Host aliasing     - One hostname (canonical name) -&gt; multiple aliases     - DNS is invoked by application to obtain cname for a supplied alias 2. Mail server aliasing     - Similar to host aliasing, DNS is invoked by mail application to obtain cname for a supplied alias. 3. Load distribution     - Busy servers are often replicated over many servers to offload heavy traffic over to other servers. Each server runs on different end systems having different IP addresses.     - Client makes a query for a name which is mapped to several addresses, the server returns a set of addresses.      - The server follows a mechanism where for each query it rotates the set as the client will typically pick the top address in the set.</p> <p>Web server and Mail server can have the same canonical name</p>"},{"location":"notes/Application%20Layer/#overview-of-dns-working","title":"Overview of DNS working","text":"<ul> <li>Application invokes the DNS server to resolve a host name.</li> <li>DNS takes over and sends a request to the network</li> <li>All requests &amp; messages are done over UDP on port 53</li> <li>DNS server sends a reply to DNS in host side with the required mapping. </li> <li>Mapping is passed to the invoked application</li> </ul> <p>All the DNS mapping arent on one server. - Maintenance - Traffic/Load on one server - Single point of failure - Distant database of mappings</p>"},{"location":"notes/Application%20Layer/#distributed-hierarchy-system","title":"Distributed Hierarchy System","text":"<p>Since one server can't handle/have all the mappings present, there are multiple servers scattered around the world. The servers are designed in an hierarchical manner.</p> <ol> <li>Root DNS Servers<ul> <li>About 400 servers</li> <li>Provides with the IP addr of the tld server</li> </ul> </li> <li>Top Level Domain Servers<ul> <li>Each tld has it's own tld server</li> <li>Provides with the IP addr of the auth server</li> </ul> </li> <li>Authoritative Servers<ul> <li>Each organization with a public host name has to have its IP address publicly listed.</li> <li>Orgs can have their own servers or pay money to a service provider for the same.</li> </ul> </li> </ol> <p>Local DNS Server: Doesn't belong to the hierarchical system but is central to the DNS structure. ISP's set up their own local DNS servers. The addresses of these local servers is sent to the host when a host connects to the ISP. When a host makes a DNS query, the query is sent to the local DNS server, which acts a proxy, forwarding the query into the DNS server hierarchy.</p>"},{"location":"notes/Application%20Layer/#example","title":"Example","text":"<p><code>cse.nyu.edu</code> wants IP addr of -&gt; <code>gaia.cs.umass.edu.</code></p> <p>What we know, - Local DNS for <code>cse.nyu.edu</code> -&gt; <code>dns.nyu.edu</code> - Auth DNS for <code>gaia.cs.umass.edu</code> -&gt; <code>dns.umass.edu</code></p> <p>Flow =&gt;</p> <pre><code>[Host: cse.nyu.edu]\n     |\n     | (1. Send DNS Query for \"gaia.cs.umass.edu\" to Local DNS Server)\n     v\n[Local DNS Server: dns.nyu.edu]\n     |\n     | (2. Forward Query to Root DNS Server)\n     v\n[Root DNS Server]\n     |\n     | (3. Respond with IP Addresses of TLD Servers for \"edu\")\n     v\n[Local DNS Server: dns.nyu.edu]\n     |\n     | (4. Forward Query to TLD DNS Server for \"edu\")\n     v\n[TLD DNS Server for \"edu\"]\n     |\n     | (5. Respond with IP Address of Authoritative DNS Server for \"umass.edu\")\n     v\n[Local DNS Server: dns.nyu.edu]\n     |\n     | (6. Forward Query to Authoritative DNS Server: dns.umass.edu)\n     v\n[Authoritative DNS Server: dns.umass.edu]\n     |\n     | (7. Respond with IP Address of \"gaia.cs.umass.edu\")\n     v\n[Local DNS Server: dns.nyu.edu]\n     |\n     | (8. Send IP Address of \"gaia.cs.umass.edu\" back to Host: cse.nyu.edu)\n     v\n[Host: cse.nyu.edu]\n     |\n     | (9. Use IP Address to connect to \"gaia.cs.umass.edu\")\n     v\n[Establish Connection with \"gaia.cs.umass.edu\"]\n\n</code></pre> <p>The above requests are both iterative &amp; recursive.</p> <ul> <li>Iterative -&gt; All except 1</li> <li>Recursive -&gt; 1(since cse makes a request to dns for host &amp; dns resolves it all on its server)</li> </ul> <p>![[Pasted image 20240826201041.png]]</p> <p>Here\u2019s a comparison of iterative and recursive DNS requests in table form:</p> Feature Iterative DNS Requests Recursive DNS Requests Client Involvement High; client queries multiple servers sequentially. Low; client delegates all queries to the DNS server. Caching Primarily at the resolver/server level; limited client-side caching. Local caching at the DNS resolver to speed up future requests. Process Flow Client receives referrals from servers and queries each one until a response is obtained. Resolver queries other servers on behalf of the client until a complete answer is found. Resolving Server Behavior Server provides referrals to other servers without fetching the complete answer. Server performs all necessary queries and returns the complete answer or an error. Efficiency Can be less efficient due to multiple queries by the client. More efficient for the client as it requires fewer actions on their part. Use Case Useful in scenarios where clients need to manage their own queries or when using lightweight resolvers. Commonly used in most DNS lookups, especially in user-facing applications where ease of use is essential."},{"location":"notes/Application%20Layer/#dns-caching","title":"DNS caching","text":"<p>DNS caching plays a big role in DNS resolving. It reduces delay performance and also reduces the traffic in the network.</p> <p>DNS caching is what allows DNS servers to learn/cache IP addresses. It caches the mapping in its local memory. Since these mappings are never permanent, the saved addresses are discarded after a while.</p>"},{"location":"notes/Application%20Layer/#dns-records-and-messages","title":"DNS records and Messages","text":""},{"location":"notes/Application%20Layer/#records","title":"Records","text":"<p>Servers that implement DNS distributed servers also store resource records (rr). Each DNS reply carries one or more dns records</p> <ul> <li> <p>Structure: <code>(Name, Value, Type, TTL)</code></p> <ul> <li>TTL: time to live of the rr.</li> </ul> </li> <li> <p>Type = A</p> <ul> <li>Name is the hostname</li> <li>Value is the ip address of the host name</li> <li>Standard hostname to IP addr mapping</li> </ul> </li> <li>Type = NS<ul> <li>Name is domain</li> <li>Value is the hostname of auth dns server</li> <li>Used to route dns queries further down the chain</li> </ul> </li> <li>Type = CNAME<ul> <li>Name is the alias domain</li> <li>Value is the IP addr of the canonical name</li> </ul> </li> <li>Type = MX<ul> <li>Name is the email alias domain</li> <li>Value is the IP addr of the canonical email name</li> </ul> </li> </ul> <p>Servers can have the same alias for web servers and mail servers. To obtain either we use the specific type as mentioned above.</p>"},{"location":"notes/Application%20Layer/#messages","title":"Messages","text":"<p>The query and reply sent by a DNS server follow a similar pattern</p> <ol> <li>Header Section (12 bytes)<ul> <li>16 bit identifier identifies the query</li> <li>Flags such as, 1 bit query(0)/reply(1), 1 bit auth flag, 1 bit recursion-desired flag, 1 bit recursion-available flag</li> </ul> </li> <li>Question section (variable)<ul> <li>Name field which contains the name of the thing that is being queried.</li> <li>Type field which tells about the type of the question being asked about the name (A, MX, etc.).</li> </ul> </li> <li>Answer section (Variable)<ul> <li>Contains the resource records for the name that was queried</li> <li>May contain multiple rr's</li> </ul> </li> <li>Authority section (variable)<ul> <li>Contains records of authoritative servers</li> </ul> </li> <li>Additional section (variable)<ul> <li>Additional info such as CNAME's of email/hostname aliases.</li> </ul> </li> </ol> <p>![[Pasted image 20240826223758.png]]</p> <p>To illustrate the structure of a DNS query and response, consider a request to resolve the domain name\u00a0<code>example.com</code>.\u00a0Table below\u00a0shows the interpretation of the bytes of the request. (Note that the exact structure of the UDP datagram consists of just the bytes shown, concatenated in order:\u00a0<code>123401000001...</code>) The header starts with a 16-bit randomly chosen identifier denoted as XID (<code>1234</code>\u00a0in our example), followed by a 16-bit value that serves as a bit mask. The structure of the bit mask is shown in\u00a0Table 4.8. The rest of the header after the bit-mask indicates how many entries are in each of the other fields, each as a 16-bit value.</p> <p>Table 4.7 ![[Pasted image 20240828085815.png]]</p> <p>Table 4.8\u00a0illustrates the structure of the 16-bit flag field that follows the XID field of a DNS header. In the message shown in\u00a0Table 4.7, the only bit set is the\u00a0<code>Recursion\u00a0Desired</code>\u00a0(<code>RD</code>) bit. The\u00a0<code>Opcode</code>\u00a0field indicates that this is a standard query (<code>SQUERY\u00a0=\u00a00000</code>). In the response shown in\u00a0Table 4.9, the flag value is\u00a0<code>0x8180</code>, which means that the Query Response (<code>QR</code>) bit has been set to indicate the message is a response, as well as the\u00a0<code>Recursion\u00a0Available</code>\u00a0(<code>RA</code>) bit. The\u00a0<code>RCODE</code>\u00a0field is used to indicate if an error occurs, and all 0 bits there indicates there was no error processing the query. Information on the other fields is available in RFC 1035.</p> <p>![[Pasted image 20240828085910.png]]</p> <p>Table 4.9\u00a0shows the response for the query from\u00a0Table 4.7. In the response, the\u00a0<code>header</code>\u00a0is almost identical to that of the request. The randomly chosen identifier\u00a0<code>XID</code>\u00a0should match the original request; if the resolver has sent multiple requests, the\u00a0<code>XID</code>\u00a0field allows the resolver to determine which request is being answered. The bit mask has been modified to denote that this message is a response and the recursive resolution strategy is available. The\u00a0<code>header</code>\u00a0also indicates that a single answer has been provided. The\u00a0<code>question</code>\u00a0field is identical to the original request.</p> <p>![[Pasted image 20240828090348.png]]</p>"},{"location":"notes/Application%20Layer/#inserting-records-in-dns-db","title":"Inserting records in DNS DB","text":"<ol> <li>Register your domain through the registrar (requires a small fee to be paid). Registrar ensure uniqueness.</li> <li>Provide IP addresses of 1st, 2nd, (so on) auth DNS servers. For each server there would be a Type A and Type NS record in the TLD servers.</li> <li>Make sure to have Type MX and A rr's entered in your auth DNS server.</li> </ol> <p>4.6. UDP Socket Programming: DNS \u2014 Computer Systems Fundamentals (jmu.edu)</p>"},{"location":"notes/Application%20Layer/#socket-programming","title":"Socket Programming","text":""},{"location":"notes/Computer%20Networks%20and%20the%20Internet/","title":"What is the Internet","text":""},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#nuts-and-bolts-description","title":"Nuts and Bolts Description","text":"<p>The internet is a computer network that interconnects billions of computing devices throughout the world. On the internet, all of these devices are called hosts or end systems.</p> <p></p> <p>End systems are connected to each other by a network of communication links (coaxial, copper, optical, etc.) and packet switches. Different links can transmit data at different rates.</p> <p>Transmission rate of a link is measured in bits/second.</p> <p>When data is to be transferred, it isn't just sent as raw data; it's modified and merged with additional information required by both end systems.</p> <p>Sending data system =&gt; Data + Header bytes (packets)</p> <p>The receiving end system receives the packets and de-structures them to get the original data.</p> <p>How does this packet reach the receiver end system though? Something called a packet switch is used to forward the packet received and send it to the appropriate end system based on the headers present.</p> <p>The two most prominent types of packet switches are Routers and Link-Layer switches.</p> <p>Routers =&gt; Access networks (An access network is a type of network which physically connects an end system to the immediate router (also known as the \u201cedge router\u201d) on a path from the end system to any other distant end system.)</p> <p>Link Layer Switches =&gt; Network core (Part of a telecom system that connects users and the network)</p> <p>The sequence of communication links and switches that are traversed by a packet is known as a route or path.</p> <p>The internet provided to the End systems allows the transfer of data. The internet is provided through Internet Service Providers (ISPs).</p> <p>Essentially, each ISP in itself is a network of packet switches and communication links.</p> <p>Examples of some network access provided by ISPs are Cable modem/DSL &amp; mobile wireless access.</p> <p>Now there are multiple tiers of ISPs that have to be interconnected to provide internet access.</p> <ol> <li>Lower Tier</li> <li>National Tier</li> <li>International Upper Tier</li> </ol> <p>Each of these ISPs are managed and runs the IP Protocol and adheres to the standards that are set by some organizations.</p> <p>The IP protocol mentioned above isn't the only protocol, and it isn't only run by ISPs. End systems, switches, and other devices present on the internet also run the protocols.</p> <p>The two most important protocols on the internet are: 1. Transmission Control Protocol (TCP) 2. Internet Protocol (IP)</p> <p>These two protocols are collectively called TCP/IP.</p> <p>As for the standards mentioned, the Internet standards are developed by the Internet Engineering Task Force (IETF).</p> <p>The IETF standards documents are called Request for Comments (RFCs). RFCs define protocols such as TCP, IP, HTTP, etc. There are currently 7,000 RFCs.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#services-description","title":"Services Description","text":"<p>The previous section described the internet using its components. But we can also describe it as an infrastructure that provides services to applications.</p> <p>Traditional applications (email, web surfing, messaging, maps, etc.) are said to be distributed applications since they involve multiple end systems that exchange data with each other.</p> <p>The internet primarily runs on the end systems, and the packet switches help with the exchange of data among end systems.</p> <p>But how do these applications run on the end systems? To even make an application first, we need to write it using a language such as C, JAVA, etc. </p> <p>These programs can be run on end systems independently, but for these programs to transfer data and communicate, we need something called a socket interface, which is provided by the Internet.</p> <p>The socket interface specifies how a program on the end systems asks the Internet to deliver data to a specific destination.</p> <p>Again, this socket interface also has to follow a set of rules that the sending program must follow.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#what-is-a-protocol","title":"What is a Protocol?","text":"<p>A protocol is exactly what it says; all activity on the internet that involves two or more communicating remote entities is governed by a protocol.</p> <p>A simple example where a protocol is used is on the Web. When we first type the URL of a Web page into a web browser, the computer sends a request to the server. The server receives this and sends an OK for the connection. Following this, the computer then sends a GET request to the server to load the web page. At last, the server returns the web page to the computer.</p> <p>A protocol defines the format and the order of messages exchanged between two or more communicating entities, as well as the actions taken on the transmission and/or receipt of a message or other event.</p> <p></p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#network-edge","title":"network edge","text":"<p>Internets end systems includde desktop computers, servers &amp; mobile devices. End systems are also referred to as hosts. Usually they are further divided into client and server. client -&gt; user desktops &amp; etc, server -&gt; data centers.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#access-networks","title":"access networks","text":"<p>The network that physically connects an end system to the first router on a path from one end system to another distant end system.</p> <p>Types of broadband residential acces 1. Digital subscriber line 2. cables</p> <p>DSL, - users DSL is also used as the ISP. - DSL modems use the existing connection lines (twisted pair copper wire) to exchange data with the digital subscriber line access multiplexer (DSLAM) to send it to the local control office. - The DSL takes data -&gt; high frequency tones -&gt; CO. - The high frequency tones are then converted back to the data. - The line is encoded in three different frequences     1. High speed downstream     2. Medium speed upstream     3. Ordinary two way telephone - On customer side, Splitter splits the telephone signals and the data and forwards it to the DSL modem. - DSL standards have different upstream and downstream rates. - The rate also depends the distance between the homes and CO.</p> <ul> <li>Cable internet access makes use of existing cable telivision lines</li> <li>these cables connect to neighbourhood junctions to about 500-5,000 homes.</li> <li>since these cables provide fibre and coaxial cables, it's aka Hybrid fibre coax (HFC).</li> <li>cia requires modems. </li> <li>cable modem termination system (cmts) (similar to dslam) converts analog signal into digital.</li> <li>cable modems divide hfc into upstream and downstream with similar transfer rates. </li> <li>cia is shared broadcast medium</li> <li> <p>![[Pasted image 20240921082004.png]]</p> </li> <li> <p>fiber to the home ftth, a new emerging technology</p> </li> <li>optical fiber direct from CO.</li> <li>types -&gt; direct fiber</li> <li>one fiber split into multiple for multiple houses.</li> <li>technique -&gt; active optical network &amp; passive optical networks</li> <li>FTTH using PON architecture involves an optical network terminator at each household which is connected to a splitter. this splitter connects multiple connections to one fiber line which meets at the optical line terminator</li> <li>![[Pasted image 20240921083129.png]]</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#ethernet-and-wifi","title":"ethernet and wifi","text":"<ul> <li>Ethernet LAN is the most common access technology in corporate, university, and home networks.</li> <li>Ethernet users use twisted-pair copper wires to connect to an Ethernet switch, which is connected to the larger Internet.</li> <li>Ethernet access provides 100 Mbps or 1 Gbps for users and 1 Gbps to 10 Gbps for servers.</li> <li>Wireless LAN (WiFi) based on IEEE 802.11 technology is widely used in places like universities, offices, cafes, homes, and even airplanes.</li> <li>WiFi access is typically available within a few tens of meters from the access point.</li> <li>802.11 (WiFi) provides shared transmission rates of more than 100 Mbps.</li> <li>Ethernet and WiFi were originally deployed in enterprise settings but are now common in home networks.</li> <li>Many homes combine broadband access (via cable modems or DSL) with WiFi to create home networks.</li> <li>A typical home network consists of:</li> <li>A wireless access point (base station) for WiFi.</li> <li>A router connecting the base station and wired devices to a cable modem for Internet access.</li> <li>Devices such as laptops, wired PCs, and other wireless devices connected through the network.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#3g-and-lte","title":"3g and lte","text":"<ul> <li>Mobile devices like iPhones and Android phones are commonly used for messaging, photo sharing, watching movies, and streaming music on the go.</li> <li>These devices use cellular networks to send/receive packets through a base station operated by cellular network providers.</li> <li>Cellular networks offer a wider range than WiFi, with users needing to be within a few tens of kilometers of the base station (compared to WiFi's range of tens of meters).</li> <li>3G wireless technology provides packet-switched wide-area Internet access at speeds greater than 1 Mbps.</li> <li>4G technology, such as LTE (Long-Term Evolution), offers even higher-speed wide-area access, with reported rates exceeding 10 Mbps, and downstream speeds in commercial deployments reaching tens of Mbps.</li> <li>LTE has evolved from 3G technology and is part of the 4G wireless network generation.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#physical-media","title":"physical media","text":"<ul> <li>Network access technologies use various physical media for data transmission, such as HFC using fiber and coaxial cable, DSL and Ethernet using copper wire, and mobile networks using the radio spectrum.</li> <li>A physical medium refers to the method through which bits are transmitted via electromagnetic waves or optical pulses between transmitter-receiver pairs along a communication path.</li> <li>A bit travels through a network by passing between a series of transmitter-receiver pairs, using different types of physical media along the way.</li> <li>Common physical media include:</li> <li>Twisted-pair copper wire</li> <li>Coaxial cable</li> <li>Multimode fiber-optic cable</li> <li>Terrestrial radio spectrum</li> <li>Satellite radio spectrum</li> <li>Physical media are divided into:</li> <li>Guided media, where signals travel through a solid medium like fiber-optic cables, twisted-pair copper wire, or coaxial cable.</li> <li>Unguided media, where signals propagate through the atmosphere or outer space, such as in wireless LANs or satellite channels.</li> <li>The cost of the physical medium (wires, cables) is often much lower compared to the labor cost for installation.</li> <li>To reduce future costs, builders often install multiple media types (twisted-pair, fiber, coaxial) in buildings even if only one medium is used initially.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#network-core","title":"network core","text":""},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#packet-switching","title":"packet switching","text":"<ul> <li>end systems exchange messages with each other</li> <li>to send messages, the  message is broken donw into smaller cunks of data called packets. </li> <li>the packets are sent through comunication links &amp; packets switches.</li> <li>packets are transmitted over the links at a rate equal to the full transmission rate of the link</li> <li>i.e. L/R where L = a packet of L bits and R = transmission rate bits/sec</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#store-and-forward","title":"store and forward","text":"<ul> <li>Store-and-forward transmission: Routers store a packet\u2019s bits before forwarding it, meaning they must receive the entire packet before transmitting it to the next link.</li> <li>Example setup: A simple network has two end systems connected by a router. The source has three packets of L bits to send to the destination.</li> <li>Total delay for a single packet: The source starts transmitting at time 0, and by time L/R seconds, the router has received the full packet and can start transmitting. At 2L/R seconds, the destination has received the entire packet. Total delay for one packet is 2L/R.</li> <li>Alternative approach: If routers forwarded bits immediately as they arrived (without storing the entire packet), the delay would be reduced to L/R, since bits wouldn't need to wait at the router.</li> <li>Multiple packets scenario: When sending multiple packets:</li> <li>At L/R seconds, the router forwards the first packet and the source starts sending the second packet.</li> <li>At 2L/R seconds, the destination receives the first packet and the router receives the second.</li> <li>At 3L/R seconds, the destination receives the first two packets and the router receives the third.</li> <li>At 4L/R seconds, the destination receives all three packets.</li> <li>General case: For a path with N links, the total end-to-end delay for a single packet is determined using the same logic and increases with the number of routers (N-1 routers between source and destination).</li> <li>![[Pasted image 20240921090708.png]]</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#queuing-delays-packet-loss","title":"queuing delays &amp; packet loss","text":"<ul> <li>Output buffer (output queue): Each packet switch has an output buffer for each attached link. This stores packets waiting to be transmitted into that link.</li> <li>Buffer role in packet switching: If a packet arrives and the link is busy, the packet waits in the output buffer, leading to potential queuing delays.</li> <li>Queuing delays: These delays are variable and depend on network congestion. If the buffer is full, packet loss may occur, either dropping the arriving packet or an already-queued one.</li> <li>Packet loss: Occurs when the buffer is full due to congestion. Either the new packet or one of the queued packets is dropped.</li> <li>Figure 1.12 example: Hosts A and B send packets to Host E over 100 Mbps Ethernet links, which then funnel into a 15 Mbps link. Congestion occurs if the arrival rate exceeds 15 Mbps.</li> <li>Queuing example: If Hosts A and B each send a burst of packets simultaneously, most of the packets will wait in the output buffer queue, analogous to real-life queuing situations like at a bank teller or tollbooth.</li> <li>Queuing delay: Further analyzed in Section 1.4.</li> <li>figure 1.12![[Pasted image 20240921091050.png]]</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#forwarding-tables-and-routing-protocols","title":"Forwarding Tables and Routing Protocols","text":"<ul> <li>Router forwarding: Routers forward packets arriving on one link to another based on the packet\u2019s destination address, which is an IP address in the Internet.</li> <li>IP address structure: The IP address is hierarchical, similar to postal addresses. The source end system includes the destination IP address in the packet\u2019s header.</li> <li>Forwarding process: When a packet arrives at a router, the router examines part of the destination address and uses its forwarding table to determine the appropriate outbound link.</li> <li>Forwarding table: This table maps destination addresses (or portions of them) to outbound links. The router consults this table for each incoming packet.</li> <li>Analogy: The process is compared to someone asking for driving directions at each stop along the way, extracting portions of the destination address (city, street, house number) at each step, with routers acting as \"gas station attendants.\"</li> <li>Routing protocols: Forwarding tables are automatically set using special routing protocols, which determine the shortest path between routers and configure the tables accordingly. This is explored in Chapter 5.</li> <li>Traceroute tool: The passage encourages trying out the Traceroute tool at www.traceroute.org to see the end-to-end route of packets on the Internet.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#circuit-switching","title":"circuit switching","text":"<p>Circuit-Switched Networks: 1. Connection Establishment: A dedicated end-to-end connection (circuit) is established before communication begins. 2. Resource Reservation: The network reserves a constant transmission rate across its links for the duration of the connection. 3. Guaranteed Rate: The sender can transfer data at a guaranteed constant rate, as bandwidth is allocated specifically for that connection. 4. Example Structure: Circuit switches are interconnected by links, each having multiple circuits (e.g., four circuits per link). 5. Capacity Allocation: Each circuit gets a fraction of the link's total transmission capacity (e.g., if a link is 1 Mbps, each circuit may get 250 kbps).</p> <p>Packet-Switched Networks: 1. No Connection Establishment: Packets are sent without establishing a dedicated connection or reserving resources. 2. Dynamic Resource Usage: Link resources are used on a first-come, first-served basis, with no guarantees for bandwidth. 3. Congestion Handling: If links are congested, packets may have to wait in a buffer, leading to variable delays. 4. Best Effort Delivery: The network makes a best effort to deliver packets in a timely manner but does not guarantee delivery speed or reliability.</p> <ul> <li>A circuit in a link is implemented with either frequency-division multiplexing (FDM) or time-division multiplexing (TDM)</li> <li>FDM<ul> <li>the link dedicates a frequency band to each connection for the duration of the connection.</li> <li>The width of the band is called, not surprisingly, the bandwidth. </li> </ul> </li> <li>TDM<ul> <li>time is divided into frames of fixed duration, and each frame is divided into a fixed number of time slots.</li> <li>one time slot for every frame in the connection</li> <li>slots are dedicated for the sole use of that connection</li> </ul> </li> <li>problem<ul> <li>circuit switching is waste-ful because the dedicated circuits are idle during silent periods.</li> <li>the idle network resources (frequency bands or time slots in the links along the connection\u2019s route) cannot be used by other ongoing connections</li> </ul> </li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#network-of-networks","title":"network of networks","text":"<p>Access ISPs: 1. Connectivity Options: Access ISPs connect end systems (e.g., PCs, smartphones) using various technologies like DSL, cable, FTTH, Wi-Fi, and cellular. 2. Types of Access ISPs: Access ISPs can include universities and companies, not just telcos or cable companies.</p> <p>Network Structure Evolution: 1. Network of Networks: The Internet is described as a \"network of networks,\" requiring interconnection among access ISPs. 2. Direct Connections: A naive approach would involve each access ISP connecting directly to every other ISP, which is impractical due to cost.</p> <p>Hierarchical Structures: 1. Network Structure 1: Introduces a global transit ISP that interconnects all access ISPs but is costly and only one provider exists. 2. Network Structure 2: Multiple global transit ISPs emerge, allowing access ISPs to choose among providers based on pricing and services. 3. Network Structure 3: Incorporates regional ISPs that connect access ISPs to tier-1 ISPs, forming a multi-tier hierarchy.</p> <p>Points of Presence (PoPs) and Multi-Homing: 1. PoPs: Groups of routers where customer ISPs connect to provider ISPs, enhancing connectivity. 2. Multi-Homing: ISPs can connect to multiple providers to maintain service during provider failures.</p> <p>Peering and Internet Exchange Points (IXPs): 1. Peering: ISPs can establish direct connections to exchange traffic without fees (settlement-free), which reduces costs. 2. IXPs: Third-party facilities where multiple ISPs can peer, further facilitating efficient data exchange.</p> <p>Content-Provider Networks: 1. Network Structure 5: Introduces content-provider networks (e.g., Google) that operate their own networks to enhance service delivery and reduce costs. 2. Private Networks: Content providers connect to lower-tier ISPs directly or via IXPs while also interacting with tier-1 ISPs.</p> <ol> <li> <p>The Internet is structured as a \"network of networks.\"</p> </li> <li> <p>The text describes five increasingly complex network structures to explain Internet evolution:</p> </li> <li>Network Structure 1: A single global transit ISP connecting all access ISPs.</li> <li>Network Structure 2: Multiple competing global transit ISPs.</li> <li>Network Structure 3: A multi-tier hierarchy with tier-1 ISPs, regional ISPs, and access ISPs.</li> <li>Network Structure 4: Adds points of presence (PoPs), multi-homing, peering, and Internet exchange points (IXPs).</li> <li> <p>Network Structure 5: Incorporates content-provider networks (e.g., Google).</p> </li> <li> <p>Key concepts introduced:</p> </li> <li>Customer-provider relationships between ISPs</li> <li>Tier-1 ISPs (about a dozen exist)</li> <li>Regional and access ISPs</li> <li>Points of Presence (PoPs)</li> <li>Multi-homing (connecting to multiple provider ISPs)</li> <li>Peering (direct connections between ISPs, often settlement-free)</li> <li> <p>Internet Exchange Points (IXPs)</p> </li> <li> <p>Content provider networks (like Google) have their own global private networks, often bypassing upper-tier ISPs.</p> </li> <li> <p>The modern Internet (Network Structure 5) is complex, with diverse ISPs spanning different geographic areas and hierarchical levels.</p> </li> <li> <p>End systems connect to the Internet through access ISPs, which can use various technologies (DSL, cable, FTTH, Wi-Fi, cellular).</p> </li> <li> <p>Access ISPs can be traditional telcos, cable companies, universities, or other organizations.</p> </li> </ol> <p>![[Pasted image 20240921094013.png]]</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#delay-loss-and-throughput","title":"delay, loss and throughput","text":"<p>Packet Journey: - Path: A packet travels from a source host through routers to a destination host.</p> <p>Types of Delays: 1. Nodal Processing Delay:     - Time taken for a router or host to process the packet header and make routing decisions. 2. Queuing Delay:     - Time a packet spends waiting in a queue at a router or switch before being transmitted, affected by network congestion. 3. Transmission Delay:     - Time required to push all the packet's bits onto the link, calculated as the packet's size divided by the link's transmission rate. 4. Propagation Delay:     - Time it takes for the packet to travel through the physical medium (like fiber optic or copper cable) from one node to the next, depending on the distance and the medium's propagation speed.</p> <p>Total Nodal Delay: - The total delay experienced by a packet at each node is the sum of the above delays.</p> <p>Impact on Applications: - Network delays significantly affect the performance of various Internet applications, including search engines, web browsing, email, maps, instant messaging, and voice-over-IP.</p> <p>Importance: - Understanding these delays is crucial for grasping the dynamics of packet switching and overall computer networks.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#processing-delay","title":"Processing Delay:","text":"<ol> <li>Definition: The time taken by a router to examine a packet's header and decide its next destination.</li> <li>Components:<ul> <li>Header Examination: Analyzing the packet's header for routing information.</li> <li>Error Checking: Verifying the packet for bit-level errors that may have occurred during transmission.</li> </ul> </li> <li>Duration: Processing delays in high-speed routers are typically very short, often on the order of microseconds or less.</li> <li>Next Steps: After processing, the router queues the packet for transmission to the next hop (e.g., router B).</li> </ol> <p>Importance: - Understanding processing delays helps in analyzing overall network performance and router efficiency.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#queuing-delay","title":"Queuing Delay:","text":"<ol> <li>Definition: The time a packet spends waiting in a queue before it can be transmitted onto the next link.</li> <li>Factors Influencing Delay:<ul> <li>Queue Length: The number of packets already waiting to be transmitted affects the delay. If the queue is empty and no other packets are being transmitted, the delay is zero.</li> <li>Traffic Load: Heavy traffic with many packets in the queue results in longer queuing delays.</li> </ul> </li> <li>Duration: Queuing delays can vary widely, typically ranging from microseconds to milliseconds, depending on network conditions.</li> </ol> <p>Importance: - Understanding queuing delays is crucial for evaluating network performance and managing traffic effectively.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#transmission-delay","title":"Transmission Delay:","text":"<ol> <li>Definition: The time required to transmit all bits of a packet onto the link.</li> <li>Calculation: Transmission delay is calculated as: Transmission\u00a0Delay= <code>L/R</code><ul> <li>L: Length of the packet in bits.</li> <li>R: Transmission rate of the link in bits per second (e.g., R = 10 Mbps for a 10 Mbps Ethernet link).</li> </ul> </li> <li>Transmission Order: In a first-come-first-served manner, a packet can only be transmitted after all preceding packets in the queue have been sent.</li> <li>Duration: Transmission delays generally range from microseconds to milliseconds, depending on the packet size and link rate.</li> </ol> <p>Importance: - Understanding transmission delays is essential for evaluating overall network performance and optimizing data transfer.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#propagation-delay","title":"Propagation Delay:","text":"<ol> <li>Definition: The time it takes for a bit to travel from the sender (router A) to the receiver (router B) after being transmitted onto the link.</li> <li>Calculation: Propagation delay is calculated as: Propagation\u00a0Delay= <code>d/s</code><ul> <li>d: Distance between the two routers.</li> <li>s: Propagation speed of the link (typically between 2\u00d710^8 to 3\u00d710^8 meters/second, close to the speed of light).</li> </ul> </li> <li>Transmission Process: Once the last bit of the packet propagates to router B, all bits are stored and router B proceeds with forwarding the packet.</li> <li>Duration: In wide-area networks, propagation delays typically range from milliseconds.</li> </ol> <p>Importance: - Understanding propagation delays is crucial for analyzing the overall time it takes for data to travel across a network, impacting applications sensitive to latency.</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#end-to-end-delay","title":"end to end delay","text":"<p>![[Pasted image 20240921095124.png]]</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#protocol-layering","title":"protocol layering","text":""},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#network-protocols-and-layers","title":"Network Protocols and Layers:","text":"<ol> <li> <p>Layered Architecture: Network protocols are organized in layers, where each layer offers specific services to the layer above and relies on the services of the layer below. This structure helps organize network design and protocol functionality.</p> </li> <li> <p>Service Model: Each layer provides services through:</p> </li> <li>Actions performed within that layer.</li> <li> <p>Utilizing services from the layer directly beneath it.</p> </li> <li> <p>Examples:</p> </li> <li> <p>A transport layer might ensure reliable message delivery by detecting and retransmitting lost messages, relying on the underlying layer's basic message delivery service.</p> </li> <li> <p>Implementation:</p> </li> <li>Protocols can be implemented in software (e.g., application-layer protocols like HTTP and SMTP) or hardware (e.g., physical layer and data link layer protocols in network interface cards).</li> <li> <p>The network layer often employs a combination of both.</p> </li> <li> <p>Distributed Protocols: The functions of a protocol layer are distributed among end systems, packet switches, and other network components, meaning that each part of the network may implement aspects of the same protocol.</p> </li> </ol>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#advantages-of-layering","title":"Advantages of Layering:","text":"<ul> <li>Conceptual Clarity: Layering provides a structured way to discuss system components, facilitating understanding and communication about network functions.</li> <li>Modularity: It allows for easier updates and maintenance of individual components without affecting the entire system.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#potential-drawbacks","title":"Potential Drawbacks:","text":"<ul> <li>Redundancy: Layers might duplicate functionality, such as having error recovery in both the data link layer and transport layer.</li> <li>Cross-Layer Dependencies: Functionality at one layer may require information from another layer, violating the intended separation.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#internet-protocol-stack","title":"Internet Protocol Stack:","text":"<ul> <li>The Internet protocol stack consists of five layers:</li> <li>Physical Layer: Deals with the transmission of raw bits over a physical medium.</li> <li>Link Layer: Handles communication over a specific link, providing data framing and error detection.</li> <li>Network Layer: Responsible for routing packets across networks.</li> <li>Transport Layer: Ensures reliable end-to-end communication and data integrity.</li> <li>Application Layer: Interfaces with end-user applications and provides application-specific communication protocols.</li> </ul> <p>![[Pasted image 20240921095457.png]]</p> <p>Here's an overview of how data is transported through the various layers of the Internet protocol stack, detailing the actions taken at each layer:</p>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#overview-of-data-transport-through-layers","title":"Overview of Data Transport Through Layers","text":"<ol> <li>Application Layer:</li> <li>Function: This is where network applications and their protocols (e.g., HTTP, SMTP, FTP, DNS) operate.</li> <li> <p>Data Handling: When an application needs to send a message, it creates an application-layer message. This message is then passed to the transport layer for further processing.</p> </li> <li> <p>Transport Layer:</p> </li> <li>Function: Responsible for transporting application-layer messages between application endpoints.</li> <li>Protocols: There are two main protocols here: TCP (connection-oriented) and UDP (connectionless).<ul> <li>TCP: Provides reliable delivery, flow control, segmentation of long messages, and congestion control. The application message is broken into smaller segments.</li> <li>UDP: Offers a simpler, no-frills service without reliability or flow control.</li> </ul> </li> <li> <p>Data Handling: The transport layer adds headers to the application message, creating a segment (TCP/UDP packet), which includes the source and destination ports.</p> </li> <li> <p>Network Layer:</p> </li> <li>Function: Responsible for routing packets (called datagrams) from the source host to the destination host across multiple networks.</li> <li>Protocols: Primarily uses the IP protocol, which defines the format of datagrams and handles addressing and routing.</li> <li> <p>Data Handling: The network layer receives the transport layer segment, adds its own header (including source and destination IP addresses), and packages it into a datagram.</p> </li> <li> <p>Link Layer:</p> </li> <li>Function: Responsible for transferring datagrams between directly connected nodes (e.g., from one router to the next).</li> <li>Protocols: Includes various protocols like Ethernet, WiFi, and others, which may provide reliable delivery over a single link.</li> <li> <p>Data Handling: The link layer receives the datagram from the network layer, adds a link-layer header (and possibly a trailer for error detection), creating a frame.</p> </li> <li> <p>Physical Layer:</p> </li> <li>Function: Concerned with the physical transmission of bits over the communication medium (e.g., electrical signals, light pulses).</li> <li>Data Handling: The physical layer takes the frames from the link layer and transmits the individual bits over the network medium to the next node.</li> </ol>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#data-flow-summary","title":"Data Flow Summary","text":"<ul> <li>From Application to Transport: The application generates a message and hands it to the transport layer, which segments it if necessary.</li> <li>From Transport to Network: The transport layer adds a header to create a segment, passing it to the network layer.</li> <li>From Network to Link: The network layer wraps the segment in a datagram, adding its own header, and sends it to the link layer.</li> <li>From Link to Physical: The link layer encapsulates the datagram in a frame and sends it to the physical layer, which transmits the bits across the medium.</li> </ul>"},{"location":"notes/Computer%20Networks%20and%20the%20Internet/#routing-through-layers","title":"Routing Through Layers","text":"<ul> <li>At each node (host or router), this process repeats: the physical layer receives bits, the link layer decodes the frame and extracts the datagram, the network layer processes the datagram and determines the next hop, and so on, until the data reaches its final destination.</li> </ul>"},{"location":"notes/Ethernet%20Cables/","title":"Ethernet Cables","text":"<p>These cables are the reason we can do things on the internet.</p> <p>A standard Ethernet cable consists of 4 twisted pairs of wires within an outer covering. But why twisted?</p> <ul> <li>In the real world, there is a lot of Electromagnetic Interference (EMI) that can disrupt the signals traveling through these wires.</li> <li>Another source of interference is the neighboring wires themselves. If the wires were laid out in parallel, there would be signal interference between them, known as crosstalk.</li> </ul> <p>These types of cables are called Unshielded Twisted Pair (UTP) cables. There are also Shielded Twisted Pair (STP) cables, where the twisted pairs are shielded with an extra coating to provide additional protection.</p> <p>Modern Ethernet cables typically contain 4 pairs of twisted wires, all color-coded\u2014specifically orange, blue, green, and brown. These pairs of wires transmit and receive signals, enabling communication.</p> <p>This type of Ethernet is called Cat5e Ethernet cable, designed for 1000Base-T networks, which refers to twisted pairs of wires capable of transmitting data at 1000 Mbps (1 Gbps).</p> <p>Cat5e refers to the specific category of twisted pair cables used in Ethernet.</p>"},{"location":"notes/Ethernet%20Cables/#internals-of-ethernet-cable","title":"Internals of Ethernet Cable","text":"<p>Before the 8-wire cables, Ethernet used 4-wire cables, such as in the 10Base-T and 100Base-TX standards.</p> <p>The 10Base-T Ethernet cable consisted of just 2 pairs of twisted wires (or 4 wires total).</p> <p>One pair was designated for sending data, while the other was for receiving data. Data is transferred by fluctuating the voltage, representing binary data (1's and 0's), along with encoding/decoding techniques.</p> <p>However, the cables alone aren't sufficient. A connector on both ends of the cable, called an RJ-45, completes the circuit by providing metal pins. Each pin on the connector has a specific function, and the way the wires are attached to the pins determines the cable's behavior.</p>"},{"location":"notes/Ethernet%20Cables/#common-cable-configurations","title":"Common Cable Configurations","text":"<ol> <li> <p>Straight-Through Cable</p> <ul> <li>In this design, the wires are connected in the same sequence on both ends of the cable. For instance, if a wire is attached to pin 1 on one end, it will also be attached to pin 1 on the other end.</li> <li>This configuration is ideal for connecting two different types of devices (e.g., a computer to a switch).</li> <li>![[Pasted image 20240829172955.png]]</li> <li> <p>Crossover Cable</p> </li> <li> <p>In this design, the wires are crossed and connected to different pins on the opposite ends (following a specific pattern).</p> </li> <li>This configuration is useful when connecting two similar devices (e.g., computer to computer).</li> <li>![[Pasted image 20240829172943.png]]</li> </ul> </li> </ol> <p>In newer Ethernet cables, like the 1000Base-T, there are 8 wires, but only 4 of them are used for simultaneous data transmission and reception.</p> <p>These modern Ethernet cables also follow the same cable configurations as discussed above.</p> <p>There are also naming conventions for the wiring arrangements, known as T568A and T568B. By using a combination of these standards, we can create the desired cable configuration (straight-through or crossover), for example, using T568A on one end and T568B on the other for a crossover cable. ![[Pasted image 20240829173255.png]]</p> <p>Modern switches and Ethernet devices have a feature called Auto MDI-X, which allows them to automatically configure the pins, enabling the use of straight-through cables regardless of the devices being connected.</p>"},{"location":"notes/Hybrid-Cloud/","title":"Hybrid Cloud","text":"<p>On-premise data centers have been around for a long time, providing businesses with the ability to manage and control their own IT infrastructure. However, these centers can be quite expensive to maintain, which is why many organizations are increasingly opting to move to the cloud.</p> <p>The public cloud offers the same, and often more, features as traditional on-premise data centers, but at a lower cost and with greater flexibility. In recent years, there has been a shift in software development from building large, monolithic applications to creating smaller, more manageable applications known as microservices. Microservices make it easier to write, update, and deploy code. Each microservice operates within its own container, often managed by services like Kubernetes (K8s).</p> <p>While this approach has many benefits, such as scalability and ease of management, there are still challenges when it comes to security, speed, and compliance. This is where the concept of a hybrid cloud comes in. A hybrid cloud environment combines on-premise infrastructure with cloud resources, allowing organizations to take advantage of both.</p> <p>However, hybrid cloud setups come with their own set of disadvantages:</p> <ol> <li>Cost: Managing both on-premise and cloud environments can be more expensive.</li> <li>Learning Curve: Engineers familiar with on-premise systems need to learn about cloud technologies.</li> <li>Feature Discrepancy: Certain features available in the cloud might not be available on-premise, and vice versa.</li> </ol> <p>To address these challenges, Dell and VMware have collaborated to create servers with cloud-like features built in. VMware Cloud Foundation is integrated into Dell servers, enabling organizations to perform both on-premise and cloud operations seamlessly. This same cloud foundation is also integrated into various cloud services, simplifying management and operations across environments.</p> <p>Software-Defined Data Centers (SDDC) further enhance this by allowing automation of various tasks. SDDCs bring cloud-like features to on-premise servers, making it easier to manage resources and create containers using Kubernetes.</p>"},{"location":"notes/IP%20addresses/","title":"IP addresses","text":"<p>When IP addresses were first created, they looked like this: <code>255.255.255.255</code>. This is called an IPv4 address, which consists of 4 octets separated by 3 dots. Each octet can range from 0 to 255.</p> <p>This IP address configuration allowed for over 4 billion possible addresses to be assigned to devices worldwide. However, due to inefficient management of these addresses, we eventually ran out of them. Let\u2019s look at how this happened.</p>"},{"location":"notes/IP%20addresses/#ip-address-classes","title":"IP Address Classes","text":"<p>IP addresses are classified into different classes, with each class having a specific range of addresses. The classes are A, B, C, D, and E.</p> Class Range Subnet Mask No. of Networks A 0 - 127 255.0.0.0 128 B 128 - 191 255.255.0.0 16,384 C 192 - 223 255.255.255.0 2,097,152 D 224 - 239 N/A N/A E 240 - 255 N/A N/A <p>The subnet mask defines which part of the IP address is the network portion and which part is the host portion. The octets that match <code>255</code> in the subnet mask are fixed and define the network, while the octets that don\u2019t match <code>255</code> can vary and define individual hosts within that network.</p> <p>Class A addresses, which provide a large number of host addresses, are typically assigned to large organizations and government entities.</p> <p>When an IP address conforms to the class and subnet mask specifications, it is part of a Classful network. However, if we take an IP address and apply a different subnet mask than the default for its class, it belongs to a Classless network.</p> <p>For example, the IP address <code>9.0.0.0</code> with a subnet mask of <code>255.0.0.0</code> is in a classful network. But if we take an address from this network, say <code>9.1.4.0</code>, and assign it a subnet mask of <code>255.255.255.0</code>, it becomes part of a classless network.</p> <p>The subnet mask also tells us how many networks and how many hosts per network we can have. Class C provides the most networks with fewer hosts per network compared to Classes A and B.</p> <p>Class D is used for multicasting, while Class E is reserved for experimental purposes and is not commonly used.</p> <p>The range <code>127.0.0.0</code> is reserved as a loopback address, which is used for testing and diagnostics on a local machine. The most common loopback address is <code>127.0.0.1</code>, often referred to as \"localhost.\" This means we have about 16 million addresses dedicated to testing purposes.</p>"},{"location":"notes/IP%20addresses/#private-ip-addresses","title":"Private IP Addresses","text":"<p>As the available IPv4 addresses began to run out, RFC 1918 was introduced to help mitigate the issue.</p> <p>The solution was to designate certain ranges from the existing classes (A, B, and C) as private addresses. These private IP address ranges are:</p> <ul> <li>Class A: 10.0.0.0 - 10.255.255.255 (Subnet Mask: 255.0.0.0)</li> <li>Class B: 172.16.0.0 - 172.31.255.255 (Subnet Mask: 255.240.0.0)</li> <li>Class C: 192.168.0.0 - 192.168.255.255 (Subnet Mask: 255.255.0.0)</li> </ul> <p>Private IP addresses are used within local networks and are not routable on the public internet. This means that multiple devices across different networks can have the same private IP addresses without causing conflicts.</p> <p>However, devices within a local network using private IP addresses need a way to communicate with the internet. This is achieved through Network Address Translation (NAT), a process performed by your router.</p> <p>When a device with a private IP wants to communicate with the internet, the router uses a public IP address provided by your ISP. NAT translates the private IP to the public IP, allowing the device to communicate externally. When a response is received, the router uses NAT to direct the response to the correct device within the local network.</p> <p>Even with NAT, the exhaustion of IPv4 addresses remained a problem, leading to the development and adoption of IPv6, which provides a much larger address space.</p>"},{"location":"notes/Linux%20networking%20commands/","title":"Linux networking commands","text":"<ol> <li><code>ifconfig</code>:</li> <li>Description: Displays or configures network interface parameters for a network using the Internet Protocol. It shows information about network interfaces such as IP addresses, MAC addresses, and network statistics.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909002730.png]]</p> </li> <li> <p><code>ip</code>:</p> </li> <li>Description: A versatile tool for network management that replaces older tools like <code>ifconfig</code>. It is used to show and manipulate routing, devices, policy routing, and tunnels.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909002823.png]]</p> </li> <li> <p><code>traceroute</code>:</p> </li> <li>Description: Shows the route packets take from one host to another. It lists each hop along the path and helps in diagnosing network routing issues.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003333.png]]</p> </li> <li> <p><code>tracepath</code>:</p> </li> <li>Description: Similar to <code>traceroute</code>, it traces the network path to a target host and displays the route packets take, including any network issues or packet loss.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003540.png]]</p> </li> <li> <p><code>ping</code>:</p> </li> <li>Description: Sends ICMP Echo Request messages to a host to check its reachability and measure round-trip time. It's useful for checking if a network device is reachable and how long it takes to communicate.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003617.png]]</p> </li> <li> <p><code>netstat</code>:</p> </li> <li>Description: Displays network connections, routing tables, interface statistics, masquerade connections, and multicast memberships. It's useful for network troubleshooting and monitoring.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003641.png]]</p> </li> <li> <p><code>nslookup</code>:</p> </li> <li>Description: Queries DNS to obtain domain name or IP address mapping. It helps in diagnosing DNS-related issues by providing details about DNS records.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003658.png]]</p> </li> <li> <p><code>dig</code>:</p> </li> <li>Description: A flexible tool for querying DNS nameservers to retrieve domain name information. It provides detailed information about DNS records and is often used for troubleshooting DNS issues.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003716.png]]</p> </li> <li> <p><code>route</code>:</p> </li> <li>Description: Displays or modifies the IP routing table. It is used to view or configure routes for network traffic and can help in diagnosing routing issues.</li> <li> <p>![[CCNA/cn-archive/docs/images/Pasted image 20240909003731.png]]</p> </li> <li> <p><code>host</code>:</p> <ul> <li>Description: Performs DNS lookups to resolve domain names to IP addresses and vice versa. It's a simpler alternative to <code>dig</code> for querying DNS information.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909003756.png]]</li> </ul> </li> <li> <p><code>arp</code>:</p> <ul> <li>Description: Displays and modifies the ARP (Address Resolution Protocol) cache. This cache maps IP addresses to MAC addresses on a local network.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909003833.png]]</li> </ul> </li> <li> <p><code>iwconfig</code>:</p> <ul> <li>Description: Configures wireless network interfaces. It provides information about the wireless connection, such as signal strength and access points.<ul> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909003903.png]]</li> </ul> </li> </ul> </li> <li> <p><code>curl</code>:</p> <ul> <li>Description: Transfers data from or to a server using various protocols (HTTP, HTTPS, FTP, etc.). It\u2019s used for interacting with web services and APIs.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909004050.png]]</li> </ul> </li> <li> <p><code>wget</code>:</p> <ul> <li>Description: A command-line utility for downloading files from the web. It supports HTTP, HTTPS, and FTP protocols and can download multiple files and directories recursively.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909004150.png]]</li> </ul> </li> <li> <p><code>telnet</code>:</p> <ul> <li>Description: Provides a command-line interface for communicating with a remote host using the Telnet protocol. It is used for testing and debugging network services.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909005511.png]]</li> </ul> </li> <li> <p><code>whois</code>:</p> <ul> <li>Description: Queries domain registration information from a WHOIS database. It provides details about domain ownership, registration, and contact information.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909005829.png]]</li> </ul> </li> <li> <p><code>ifplugstatus</code>:</p> <ul> <li>Description: Displays the status of network interfaces and their connection state. It is useful for checking if an Ethernet cable is plugged in or not.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909005906.png]]</li> </ul> </li> <li> <p><code>nload</code>:</p> <ul> <li>Description: Monitors network traffic and bandwidth usage in real-time. It provides a graphical representation of incoming and outgoing traffic on network interfaces.</li> <li>![[CCNA/cn-archive/docs/images/Pasted image 20240909005945.png]]</li> </ul> </li> <li> <p><code>mail</code>:</p> <ul> <li>Description: A command-line utility for sending and receiving email. It is used for composing, sending, and managing email messages from the terminal. </li> <li>![[Pasted image 20241003120017.png]]</li> </ul> </li> </ol>"},{"location":"notes/Network%20Architecture/","title":"Network Architecture","text":"<p>How exactly are networks designed in the real world? What are the design criteria's they follow? How are failure points avoided? These answers are all answered by the network architecture we will discuss.</p> <p>A bad system design consists of a single point of failure, where one switch connects another switch which in the end connects to multiple devices. Failure of the first switch, could lead to failure of the devices indirectly connected to it. This can be avoided by simply following some kind of architecture rules.</p> <p></p> <p>This is formally called as Daisy Chaining of Switches</p>"},{"location":"notes/Network%20Architecture/#2-tier-architecture","title":"2 Tier Architecture","text":"<p>As we saw above, failure in one device could lead to failure in other devices too. To prevent this (not completely), we make use of the 2 Tier architecture.</p> <p>What are the two tiers? 1. Access layer 2. Distribution layer</p> <p>Here instead of one switch connecting to other, we introduce a Multi layer switch. Now unlike normal switches, the multi layer switch works in L3 layer, i.e. it deals with IP-addresses. It's almost similar to routers, but routers can't send out packets like a switch.</p> <p>Using this new multi layer switch, our smaller connections which include a switch and other devices connected to it (forms the access layer) are connected to the multi layer switch and this connects to the router (the switch and router form the distribution layer).</p> <p>Now this is significantly better than what we had before, but we still have a single point of failure.</p> <p></p>"},{"location":"notes/Network%20Architecture/#3-tier-architecture","title":"3 Tier Architecture","text":"<p>Now someone decided to add another complicated layer of single point of failure, Great. This layer is almost similar to 2 tier, but this one contains 3 layers.</p> <p>What are the layers? 1. Access Layer 2. Distribution Layer 3. Core Layer</p> <p>The access layer and the distribution layer are still the same, but the distribution layer, or the multi layer switches of the dist. layer are connected to more multi layer switches, specifically 2. These are the backbones of the architecture. These new switches are then connected to two routers.</p> <p>This architecture is used by large enterprises as they are quite expensive compared to our previous architectures.</p> <p></p>"},{"location":"notes/Network%20Architecture/#collapsed-2-tier-architecture","title":"Collapsed 2 Tier Architecture","text":"<p>Now smaller enterprises wanted something similar to the 3 tier arch. but something more inexpensive. The collapsed 2 tier architecture solves that. I</p> <p>t's almost similar to the normal 2 tier architecture but instead of one multi layer switch in the distribution layer, it contains two. This reduces the single point of failure, which is why it's also more ideal than the normal 2 tier architecture.</p> <p></p>"},{"location":"notes/Network%20Layer/","title":"Network Layer","text":"<p>Index</p> <ul> <li>[[Network Layer#4.1 overview |Overview]]<ul> <li>[[Network Layer#4.1.1 forwarding and routing data and control planes|forwarding and routing data and control planes]]<ul> <li>[[Network Layer#control plane|control plane]]</li> <li>[[Network Layer#control plane sdn approach|sdn approach]]</li> </ul> </li> <li>[[Network Layer#4.1.2 network service model|network service model]] </li> </ul> </li> <li>[[Network Layer#4.2 inside a router|Inside a Router]]<ul> <li>[[Network Layer#4.2.1 input processing &amp; destination based forwarding|input processing &amp; destination based forwarding]]</li> <li>[[Network Layer#4.2.2 Switching fabric|switching fabric]]</li> <li>[[Network Layer#4.2.3 output processing|output processing]]</li> <li>[[Network Layer#4.2.4 queueing|where does queuing occur]]</li> <li>[[Network Layer#4.2.5 packet scheduling|packet scheduling]]</li> </ul> </li> <li>[[Network Layer#4.3 ip thing|The Internet Protocol (IP): IPv4, Addressing,IPv6, and More]]<ul> <li>[[Network Layer#ipv4 datagram| IPv4 Datagram Format]]</li> <li>[[Network Layer#data fragmentation|IPv4 Datagram Fragmentation]]</li> <li>[[Network Layer#ipv4 addressing|ipv4 addressing]]</li> </ul> </li> <li>[[Network Layer#4.4 generalized forwarding &amp; sdn|Generalized Forwarding and SDN]]</li> <li>[[Network Layer#Middleboxes|Middleboxes]]</li> <li></li> </ul>"},{"location":"notes/Network%20Layer/#41-overview","title":"4.1 overview","text":"<ul> <li>transport layer -&gt; segment</li> <li>segment -&gt; network layer</li> <li>encapsulated into a datagram -&gt; sender router</li> <li>receiver router takes datagram -&gt; network layer</li> <li>breaks datagram back to segment -&gt; network layer</li> <li>routers dont run on application and transport layer protocols</li> </ul> <p>roles, - primary data-plane role of router is to forward datagrams - primary network control plane role is to coordinate these local, per router forwarding actions.</p>"},{"location":"notes/Network%20Layer/#411-forwarding-and-routing-data-and-control-planes","title":"4.1.1 forwarding and routing: data and control planes","text":"<p>network layers has two main functions</p> <ul> <li>forwarding, when a packet arrives at a router's input link, the router must move the packet to the appropriate output link. a packet can also be blocked from exiting the router<ul> <li>refers to router-local action</li> <li>implemented direcly in hardware</li> <li>takes place at very short timescale</li> </ul> </li> <li>routing, network layer must determine the route/path taken by packets as they flow from sender to receiver. the algo's used to figure out the routes are routing algorithms.<ul> <li>refers to network-wide process</li> <li>implemented in software</li> <li>takes place at large timescales</li> </ul> </li> </ul> <p>A key element of the router is the forwarding table. router examines fields of recv packet header and using these values it indexes into its table. The table helps determining the output link for this packet.</p>"},{"location":"notes/Network%20Layer/#control-plane","title":"control plane","text":"<p>routing algorithms in the control plane determine the contents of the router's forwarding table. these algo's, for now, are present in each router and each router contains the forwarding and routing functions.</p> <p>the algo's communicate with each other to share routing information through a routing protocol.</p> <p>![[Pasted image 20241003094622.png]]</p>"},{"location":"notes/Network%20Layer/#control-plane-sdn-approach","title":"control plane: sdn approach","text":"<p>in this approach, the routing algorithm, or the control plane which contains this algorithm, is built seperately from the router, this is the remote controller.</p> <p>the control plane is built in remote data centers that are controlled by ISP's or third party services. the router then communicates with this remote controller exchanging the forwarding table data and other router information.</p> <p>here, router -&gt; forwarding function, remote controller -&gt; routing function.</p> <p>This setup is known as the software defined networking approach. this is more reliable and redundant. in recent times this software has been made open sourced.</p> <p>![[Pasted image 20241003095215.png]]</p>"},{"location":"notes/Network%20Layer/#412-network-service-model","title":"4.1.2 network service model","text":"<p>the network service model defines the characteristics of end-to-end delivery between sending and receiving hosts.</p> <p>the services provided are, 1. guaranteed delivery 2. guaranteed delivery with bounded delay 3. in order packet delivery 4. guaranteed minimal bandwidth 5. security</p> <p>the network layer provides a single service known as a best effort service. this means that the network layer can never guarantee any of the above mentioned services.</p> <p>even though it doesn't guarantee the services, the internets best service model combined with adequate bandwidth provisioning is proven to be more than good enough.</p> <ul> <li>Simplicity of mechanism has allowed internet to be widely deployed adopted.</li> <li>sufficient provisioning of bandwidth allows performance of real time applications to be good enough for most of the time</li> <li>replicated, application layer distributed services connecting close to clients' network, allow services to provided from multiple </li> </ul>"},{"location":"notes/Network%20Layer/#42-inside-a-router","title":"4.2 inside a router","text":"<p>![[Pasted image 20241007135929.png]]</p> <ul> <li>input port<ul> <li>terminates an incoming physical link at a router (shown at both ends) (this is a physical layer function)</li> <li>also performs link layer functions (rep by middle boxes and input &amp; output ports)</li> <li>lookup function (rightmost box of input port). the forwarding table is consulted where to send the packet to the right output port via the switching fabric</li> <li>control packets from ip port (packets w/ routing info) -&gt; routing processor</li> </ul> </li> <li>switching fabric<ul> <li>connections ip port to op port.</li> </ul> </li> <li>output port<ul> <li>stores packets recv from s/w fabric</li> <li>sends packets to approp links using link &amp; physical layer functions.</li> </ul> </li> <li>routing processor<ul> <li>control-plane functions</li> <li>executes router protocols</li> <li>maintains tables &amp; attached link state information</li> <li>computer forwarding table</li> <li>in SDN, it communicates with the remote-controller to write down the receiving forward table entries computed by remote controller, into the routers input ports.</li> </ul> </li> </ul> <p>types of forwarding, - Destination based forwarding - General forwarding</p> <p>this architecture is almost always implemented in hardware. this is the dataplane, which operates at a nanosecond scale.</p> <p>whereas, the control plane is implemented in software operating at a millisecond or second scale.</p>"},{"location":"notes/Network%20Layer/#421-input-processing-destination-based-forwarding","title":"4.2.1 input processing &amp; destination based forwarding","text":"<p>The input port's  - line terminal function  - and link-layer processing implement the physical and link layer functionalities for that input link.  - Further, the lookup performed is central to the router's operation.</p> <p>In lookup, the forwarding table used is either computed by the routing processor or sent by the SDN remote controller. This table is copied from the routing processor -&gt; line cards over a separate bus.</p> <p>At each line card, a shadow copy is performed, this is done to reduce the centralized processing bottleneck, i.e. it doesn't invoke the routing processor on a per packet basis.</p> <p>Considering a simple case where the packet received is matched in the table and then forwarded to the appropriate link based on the match. Would this mean we would need to store details of each packet in the table? This would not be optimal.</p> <p>Routers match something called prefix of the packets destination address with the entries in table. If there is any match, the packet is forwarded to a link associated with the match.</p> <p>![[Pasted image 20241014094753.png]] In this figure, the otherwise states that if there are no matches the default interface will be 3.</p> <p>Lets say there is a case when a single packet matches with multiple entries in the table, i.e. the first 21 bits of a packet could match the 3rd entry and the first 24 bits could match the 2nd entry. In this case, the router uses the longest prefix matching rule.</p> <p>Prefix matching - n bits of a address match n bits in table Longest Prefix Matching - n bits of an address match n bits of two entries in table, then choose the longer entry.</p> <p>These lookups are essentially simple to build (loopkup - search through forwarding tables), but for larger links, they must be fast. Some chips that are used, 1. DRAM and faster SRAMs 2. Ternary Content Addressable Memories (TCAM) (returns in constant time)</p> <p>Once the packets link is determined, it's sent to the switching fabric. Here it can also be queued at the input port due to other links currently using the fabric.</p> <p>Although the most important action is the lookup, there are several other key things that need to be taken care of, 1. The physical and link layer processing 2. the packets version number, check sum and time-to-live field 3. counters used for network management</p> <p>Matching - input ports steps of lookup up a dest IP address Action - sending the packet to s/w fabric to the specified output The above terms together, match plus action abstraction, are performed in almost all network devices and not just routers.</p>"},{"location":"notes/Network%20Layer/#422-switching-fabric","title":"4.2.2 Switching fabric","text":"<p>this is the very heart of the router as this is the place the packets are switched from an input port to an output port. </p> <ol> <li>Switching via memory<ul> <li>simplest form of early routers</li> <li>earlier, cpu was the routing processor</li> <li>input/output ports were like traditional I/O devices</li> <li>i/p signaled the routing processor via interrupt</li> <li>packet copied (from ip) -&gt; processor memory</li> <li>packet's header -&gt; destination address extracted -&gt; lookup performed to get op port -&gt; packet copied into op port</li> <li>if memory bandwidth is B, then throughput is B/2</li> <li>this lookup and storing in memory is done on the input line cards</li> <li>two packets can't be forwarded at once</li> </ul> </li> <li>Switching via bus<ul> <li>input port directly forwards packet to output port (w/o routing processor intervening).</li> <li>packet is pre-pended with a switch-internal label (header).</li> <li>all o/p ports receive the packet but the packet with the matching label is kept and others are discarded</li> <li>once the packet reaches the op port, the label is also discarded</li> <li>one packet can cross the bus at a time.</li> </ul> </li> <li>Switching via an interconnection network<ul> <li>to overcome bandwidth limitation, use sophisticated network</li> <li>crossbar switch with 2N buses (N ip, N op)</li> <li>each vertical bus connects with a horizontal bus.</li> <li>two packets can cross at the same time if they have different ip &amp; op buses.</li> <li>crossbar switch is non-blocking, the packet is not blocked from reaching its output port as long as no other packet is going to that output port.</li> <li>More sophisticated interconnection networks use multiple stages of switching elements to allow packets from different input ports to proceed towards the same output port at the same time through the multi-stage switching fabric.</li> <li>Non-Blocking Switching:<ul> <li>The Cisco CRS router uses a \"three-stage non-blocking switching\" method.</li> <li>Non-blocking means that packets can always find a path to their destination without getting stuck due to other traffic.</li> </ul> </li> <li>Scaling Switching Capacity:<ul> <li>A router can increase its capacity by using multiple switching fabrics in parallel.</li> <li>This parallel setup allows for greater data handling capabilities.</li> </ul> </li> <li>Packet Splitting and Reassembly:<ul> <li>In the parallel fabric approach, an input port divides a packet into smaller pieces or \"chunks.\"</li> <li>These chunks are then distributed (\"sprayed\") across multiple switching fabrics (K chunks sent through N fabrics).</li> <li>At the output port, these chunks are reassembled into the original packet for delivery.</li> </ul> </li> </ul> </li> </ol> <p>![[Pasted image 20241014114430.png]]</p>"},{"location":"notes/Network%20Layer/#423-output-processing","title":"4.2.3 output processing","text":"<p>the packets stored in the output ports memory are taken and forwarded to the output link. this includes queueing, de-queueing and physical &amp; link layer processing.</p>"},{"location":"notes/Network%20Layer/#424-queueing","title":"4.2.4 queueing","text":"<p>by our discussion earlier it's clear that both the input &amp; output ports have a queue where the packets are stored for time being, but lets consider a scenario where the packets increase in number and the routers memory is eventually exhausted. Due to the memory being exhausted, there could be packet loss.</p> <p>input/output transmission rate -&gt; $R_{line}$ Switching fabric transfer rate -&gt; $R_{switch}$</p>"},{"location":"notes/Network%20Layer/#input-queueing","title":"input queueing","text":"<p>if switch fabric is not fast enough to transfer all arriving packets through the fabric without delay, then input queuing occurs, i.e. packet queue up on input ports.</p> <p>considering the switching via a cross-switch, the packet won't experience any blocking unless two packets have the same destination (output ports). But there might be a case where a packet (p2) which is meant to go to, say, output port A, and the packet which is right infront of it (p1), needs to go to, say, output port B, but the port B is currently busy with another packet (pk) hence keeping p1 in waiting. This causes p2 to kept in waiting due to p1. This is called head-of-line (HOL) blocking.</p> <p>HOL - a queued packet in an input queue must wait for transfer through the fabric (even though its output port is free) because it is blocked by another packet at the head of the line (in-front of it).</p>"},{"location":"notes/Network%20Layer/#output-queueing","title":"output queueing","text":"<p>if $r_{switch}$ &gt;&gt; $r_{line}$ of the output port, then the packets sent by the swtich will be sent faster than the rate at which output port sends packets to its appropriate links.</p> <p>when there is insufficient memory, a decision to drop the packet (arriving or queued) must be made. this policy is called the drop-tail.</p> <p>at times, packets are dropped before the memory gets full, this is done to send the sender a signal for congestion. multiple policies for proactive packet dropping &amp; marking policies have been analyzed (Active Queue Management). Most widely studied AQM -&gt; Random early detection.</p>"},{"location":"notes/Network%20Layer/#425-packet-scheduling","title":"4.2.5 packet scheduling","text":"<p>question of determining the order in which queued packets are transmitted over an outgoing link.</p>"},{"location":"notes/Network%20Layer/#fifo","title":"FIFO","text":"<ul> <li>based on first come first serve basis priority</li> <li>packets are transmitted based on the order they have arrived</li> <li>the queue-packet discarding policy determines whether packet is dropped or whether packet is removed from the queue. ![[Pasted image 20241108001312.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#priority-queue","title":"priority queue","text":"<ul> <li>incoming packets are classified according to priority classes</li> <li>operators configure a queue to give higher priority to network management information packets</li> <li>similarly, voice-over-IP packets &gt;&gt; non-real traffic (SMTP, IMAP)</li> <li>the individual queues operate in fifo manner</li> <li>say a low priority packet is already under transmission when a higher priority packet arrives, the lower priority transmission is not stopped (non-preemptive priority queue). ![[Pasted image 20241123141515.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#round-robin","title":"round robin","text":"<ul> <li>queued just like in priority queue</li> <li>here classes are given equal opportunities to transmit packets, this is the work conserving queuing discipline.</li> <li>this discipline checks for packets in the current class, if it finds none, it checks the next class</li> </ul> <p>![[Pasted image 20241123141648.png]]</p>"},{"location":"notes/Network%20Layer/#weighted-fair-queuing","title":"weighted fair queuing","text":"<ul> <li>packets are classified and queued in appropriate per-class waiting area</li> <li>serve classes in circular manner (1 -&gt; 2 -&gt; 3 -&gt; 1 and so on)</li> <li>also work conserving</li> <li>each class receives a differential amount of service in any interval of time.</li> <li>class i -&gt; weight i</li> <li>in any time interval, class i -&gt; guaranteed $w_i/(\\sum w_j)$ fraction of service</li> <li>for transmission rate R, class i -&gt; guaranteed $R*w_i/(\\sum w_j)$ throughput</li> </ul> <p>![[Pasted image 20241123141952.png]]</p>"},{"location":"notes/Network%20Layer/#43-ip-thing","title":"4.3 ip thing","text":""},{"location":"notes/Network%20Layer/#ipv4-datagram","title":"ipv4 datagram","text":"<ul> <li>network layer packet is called datagram.</li> <li>version number<ul> <li>4 bits</li> <li>defines version of IP datagram</li> </ul> </li> <li>header<ul> <li>20 byte header</li> <li>defines where the payload actually starts in the datagram</li> </ul> </li> <li>type of service (tos)<ul> <li>helps differentiate between datagrams</li> <li>Example of TOS -&gt; Explicit Congestion Notification (6:7) </li> </ul> </li> <li>datagram length<ul> <li>16 bits long</li> <li>length of datagram (header + data) - measured in bytes</li> <li>max size of datagram - $2^{16} - 1$ = 65,535 bytes</li> </ul> </li> <li>identifier, flags, fragmentation offset<ul> <li>used for IP fragmentation</li> </ul> </li> <li>time to live<ul> <li>ensures that datagram isn't re-processed and doesn't circulate forever</li> <li>TTL is decremented everytime datagram is processed.</li> </ul> </li> <li>protocol<ul> <li>indicates which protocol should datagram be passed to in transport layer</li> <li>6 - TCP, 17 - UDP</li> </ul> </li> <li>header checksum<ul> <li>detects error bits</li> <li>router computes header checksum -&gt; checks if there are any errors -&gt; if errors exist datagram is discarded.</li> </ul> </li> <li>source and destination IP address</li> <li>options<ul> <li>allows header length to be extended</li> <li>complicates things as we don't know what the size of the header would be. ![[Pasted image 20241124120357.png]] <p>IP Datagram header - 20 bytes IP Datagram header w/ TCP segment -40 bytes (20 + 20 w/ application layer message)</p> </li> </ul> </li> </ul>"},{"location":"notes/Network%20Layer/#data-fragmentation","title":"data fragmentation","text":"<ul> <li>datagrams have variable size</li> <li>datagrams pass through the link layer where they are encapsulated into link layer frames.</li> <li>link layer has a maximum amount of data that can pass through it (maximum transmission unit (MTU))</li> <li>different end-systems have different link layer protocols, having different MTU's</li> <li>for the case where MTU &lt;&lt; Datagram size, we perform fragmentation of the Datagram.</li> <li>Datagram -&gt; Sub-datagrams -&gt; each sub-datagram is assigned source address, identification number (of original datagram) &amp; destination address.</li> <li>Sending host usually increments identification number for each datagram it sends.</li> <li>First sub-datagram -&gt; Flag bit: 1, Last sub-datagram -&gt; Flag bit: 0.</li> <li>Destination address checks id number to check if datagram is part of a bigger datagram. </li> <li>To order them properly, the offset field is checked</li> </ul>"},{"location":"notes/Network%20Layer/#ipv4-addressing","title":"ipv4 addressing","text":"<ul> <li>ip addresses aren't associated with devices.</li> <li>IP address are 32 bit identifiers associated with interfaces</li> <li>Interfaces: connection between host/router and physical link.</li> </ul>"},{"location":"notes/Network%20Layer/#subnets","title":"subnets","text":"<ul> <li>IP addresses must be unique. A part of the address is defined by something called subnet.</li> <li>A subnet is a network inside a network which allows device interfaces to interact without intervention of a router.</li> <li>Each isolated network is a subnet.</li> <li>IP addresses -&gt; Subnet Part (common for a given subnet) + Host Part</li> <li>Example![[Pasted image 20241124231727.png]]</li> <li>In this isolated n/w, \"223.1.3\" part is same for all devices, this is the subnet part. The \"/24\" indicates that 24 bits are the same.</li> <li>Classless Interdomain Routing (CIDR) is the slash notation, it's the subnet portion of the address of arbitrary length.</li> </ul> <p>How does the host get its address in an isolated network? How does a subnetwork get its address in an global network?</p>"},{"location":"notes/Network%20Layer/#obtaining-a-block-of-address","title":"obtaining a block of address","text":"<ul> <li>allotted by the networks ISPs address space.</li> <li>![[Pasted image 20241124234143.png]]</li> <li>The ISPs block is divided into 8 address spaces.</li> <li>These 8 organizations contact the global internet through the main ISP block addr.</li> <li>This is called address aggregation</li> <li>![[Pasted image 20241124235151.png]]</li> <li>say one organization wants to change its ISPs but also keep the address range, then the new ISP will expect its original range and also the new range from the shifted organization</li> <li>![[Pasted image 20241124235202.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#how-isps-get-a-block-of-address","title":"how ISPs get a block of address","text":"<ul> <li>provided by ICANN</li> <li>ICANN allocate IP addresses through 5 regional registries</li> <li>then RRs assign to Local registries.</li> </ul>"},{"location":"notes/Network%20Layer/#obtaining-a-host-address","title":"obtaining a host address","text":"<ul> <li>protocol used is Dynamic Host Configuration Protocol (DHCP) which allows automatic configuration of IP addresses to devices.</li> <li>This method is far better and efficient then earlier methods where the sysadmin used to hard-code the address in the config file</li> <li>when device joins a n/w, for it (the device) to get an ip addr,<ul> <li>n/w needs to have DHCP server. The host requests and receives an addr through this server by using DHCP protocol.</li> <li>when the device leaves, the IP addr is given up. (Can be re-used/reclaimed).</li> </ul> </li> </ul> <p>DHCP server runs on port 67 and UDP</p> <ul> <li>Four types of msg's b/w an incoming device and DHCP server<ul> <li>DHCP discover<ul> <li>msg broadcasted by incoming device to check if DHCP server exists</li> </ul> </li> <li>DHCP offer<ul> <li>msg broadcasted by server containing a suggested IP address the incoming device can use.</li> </ul> </li> <li>DHCP request<ul> <li>msg broadcasted by incoming device which contains the server proposed address or device wants to renew an old address</li> </ul> </li> <li>DHCP ack<ul> <li>msg broadcasted by server which acknowledges the request and grants the device the particular ip address.</li> </ul> </li> </ul> </li> </ul> <p>In this example, DHCP -&gt; port 67, client -&gt; port 68![[Pasted image 20241124233756.png]]</p>"},{"location":"notes/Network%20Layer/#nat","title":"NAT","text":"<ul> <li>all devices within a LAN will have the ip address from a specific range of addresses, known as private addresses.</li> <li>datagrams within the local network work with the private ip's.</li> <li>but for datagrams to communicate with the global network, they use a static NAT IP address.</li> <li>Implementation<ul> <li>Outgoing datagrams: (source IP address, port #) -&gt; (NAT ip address, new port #)</li> <li>Remember in NAT translation table: every mapping of the old port number to the new port number must be done in the table.</li> <li>Incoming datagrams: (NAT ip addr, new port #) -&gt; (source ip address, port #)</li> </ul> </li> <li>![[Pasted image 20241125001113.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#ipv6","title":"ipv6","text":"<ul> <li>provided larger address space</li> <li>notion of flow (connection b/w end-points) was introduced with ipv6</li> <li>source addr and destination addr<ul> <li>32 to 128 bits</li> <li>new type of address, anycast address</li> </ul> </li> <li>flow label<ul> <li>identify datagrams between \"flows\"</li> </ul> </li> <li>priority / traffic class<ul> <li>8 bit</li> <li>to give priority to certain datagrams in a flow</li> <li>like TOS in ipv4</li> </ul> </li> <li>payload length<ul> <li>16 bit</li> <li>number of bytes in IPv6 datagram.</li> </ul> </li> <li>next header<ul> <li>identifies to which protocol the datagram is being transferred to</li> </ul> </li> <li>hop limit<ul> <li>contents are decremented by one by each router that forwards the datagram.</li> <li>if content = 0, datagram discarded</li> <li>similar to TTL</li> </ul> </li> <li>no checksum, fragmentation, identification number, flag, options</li> </ul> <p>![[Pasted image 20241125004728.png]]</p>"},{"location":"notes/Network%20Layer/#ipv4-to-ipv6","title":"ipv4 to ipv6","text":"<ul> <li>both versions try to coexist</li> <li>some are ipv4 or ipv6 or both.</li> <li>tunneling allows this to happen</li> <li>in tunneling, a datagram inside a datagram is transferred. </li> <li>ipv6 datagram inside the ipv4's payload</li> <li>say we have two ipv6 routers, which also can do ipv4 stuff\\<ul> <li>we would usually connect them using an ethernet cable, but we can also connect them using a network of ipv4 routers, this network is called the tunnel. </li> <li>the datagrams b/w ipv6 routers is passed through the tunnel.</li> <li>the datagram is an ipv4 datagram w/ the payload as the ipv6 datagram.</li> </ul> </li> <li>Physical view![[Pasted image 20241125005802.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#44-generalized-forwarding-sdn","title":"4.4 generalized forwarding &amp; sdn","text":"<ul> <li>recall match plus action</li> <li>in generalized forwarding, match-plus-action table generalizes the notion of destination based forwarding table.</li> <li>here a flow table is used. it consists of a match and a corresponding action.</li> <li>most popular implementation of this is OpenFlow</li> </ul>"},{"location":"notes/Network%20Layer/#openflow","title":"OpenFlow","text":""},{"location":"notes/Network%20Layer/#match","title":"Match","text":"<ul> <li>in OpenFlow, there are a set of header that are to be matched</li> <li>![[Pasted image 20241125112654.png]]</li> </ul>"},{"location":"notes/Network%20Layer/#action","title":"Action","text":"<ul> <li>for a given match, we can perform multiple actions</li> <li>Forwarding</li> <li>Dropping</li> <li>Modify-field</li> </ul>"},{"location":"notes/Network%20Layer/#middleboxes","title":"Middleboxes","text":"<ul> <li>Any intermediate box, that performs functions apart from normal, standard functions of an IP router on the data path between source host and destination host<ul> <li>Standard functions of an ip router -&gt; forwarding</li> <li>data path -&gt; meaning middlebox is a network layer thing</li> </ul> </li> <li>Example<ul> <li>NAT/Firewalls using generalized based forwarding</li> <li>Load Balancers (aka layer 7 switch)</li> <li>Caches</li> <li>CDN's</li> </ul> </li> <li>Is a white-box hardware which implements open APIs<ul> <li>Example is Network function virtualization</li> </ul> </li> </ul>"},{"location":"notes/Network%20Models/","title":"Network Models","text":"<p>The need of network models arose when different networks consisting of different machines could not communicate with each other.</p> <p>For example, in the 1960's, many companies developed their own networks based on the ideology of the ARPANET, but these networks could not communicate with each other as they were proprietary.</p> <p>With this need of some kind of standard model, came two models, TCP/IP and the OSI network models.</p>"},{"location":"notes/Network%20Models/#tcpip-vs-osi","title":"TCP/IP vs OSI","text":""},{"location":"notes/Network%20Models/#tcpip","title":"TCP/IP","text":"<p>The Transmission control protocol/Internet protocol (aka TCP/IP) has a layered architecture consisting of 4 layers.</p> <ol> <li>Application</li> <li>Transport</li> <li>Network</li> <li>Physical</li> </ol> <p>At times, Physical layer is divided into two layers, Physical and Data Link layer</p>"},{"location":"notes/Network%20Models/#osi","title":"OSI","text":"<p>Open system interconnect (aka OSI). This model is mostly a conceptual model used in network comms by network engineers.</p> <p>It has a 7 layers architecture where most of the layers are similar to the TCP/IP</p> <ol> <li>Application</li> <li>Presentation</li> <li>Session</li> <li>Transport</li> <li>Network</li> <li>Data link</li> <li>Physical</li> </ol> <p>The Presentation and Session layers are stuffed as one to form the application layer in the TCP/IP model.</p> <p></p>"},{"location":"notes/Network%20Models/#acronyms","title":"Acronyms","text":"<ul> <li>All People Seem To Need Data Processing</li> <li>Please Do Not Throw Sausage Pizza Away</li> </ul> <p>The process of taking data and merging it with important related info is called encapsulation.</p>"},{"location":"notes/Network%20Models/#data-traversal-through-model-layers","title":"Data traversal through model layers","text":"<p>When the user wants to send data to another device, the data has to go down all the layers and get enclosed in a figurative envelope and then go up through all the layers and get opened in each layers, one by one.</p> <p>Lets understand this in a bit detail.</p> <p>Starting from the senders side, say we want to access a website. We will need to send a request to that website. To send this request we use the HTTPS protocol.</p>"},{"location":"notes/Network%20Models/#application-layer","title":"Application Layer","text":"<p>First this request will go through the application layer. It goes in the form of data which contains the info about the request and the protocol used.</p>"},{"location":"notes/Network%20Models/#transport-layer","title":"Transport Layer","text":"<p>Now this data goes down to the transport layer. Here in the transport layer, there are two protocols we mainly use, TCP (Transmission control protocol) and UDP (user datagram protocol)</p> <p>High-level difference between the two protocols</p> TCP UDP More reliable Fast <p>In this layer (L4 or transport), along with the data, another header is attached which contains info about the protocol.</p> <p>There is info like the destination port (ex: 1234) and the source port (which can be 443 -&gt; port number for HTTPS traffic).</p> <p>This data is called segments.</p>"},{"location":"notes/Network%20Models/#network-layer","title":"Network layer","text":"<p>This segment of data is now passed down to the network layer where we further encapsulate it with headers. This header contains info about the IP addresses of the source and destination.</p> <p>This header is important for the router (the router can only understand IP addresses i.e. L3 addr.) to get directions.</p> <p>The data in this layers is also referred to as packets.</p>"},{"location":"notes/Network%20Models/#data-link-layer","title":"Data Link layer","text":"<p>The packet is now sent down to the data link layer. Here, instead of just adding a header, a trailer is also added. </p> <p>The header here contains the mac addr. of the source and destination. This is needed to give directions to the switch (the switch only understands MAC address i.e. L2 addr.)</p> <p>The data in this layer is also called frames</p>"},{"location":"notes/Network%20Models/#physical-layer","title":"Physical layer","text":"<p>Finally after all the encapsulation, we receive the frames from data link layer, and now we can send the frames to the website we wanted access.</p>"},{"location":"notes/Network%20Models/#receivers-end","title":"Receivers End","text":"<p>In the receivers side, the data isn't just taken as a frame, the frame must go through the procedure of un-encapsulation so that the routers, switches can read the address of the destination.</p> <p>One by one, up the layer, each header/trailer is removed until we reach the application layer and finally receive the data.</p>"},{"location":"notes/Routers/","title":"Routers","text":"<p>Switches are good in helping connect devices that are present in a given network range (For ex - 10.1.1.0 - 10.1.1.255), but what if we want to send frames to a device which is in another network range?</p> <p>You'd probably think that we can do this task simply by connecting two switches that belong to two different network ranges. But this is wrong. Let us see why.</p>"},{"location":"notes/Routers/#same-network-range-devices","title":"Same network range devices","text":"<p>Initially, the source needs to learn the L2 address (MAC address) of the destination, so it sends a broadcast packet to the switch, since this is a broadcast packet, all the devices connected receive this packet. The device which was actually meant to receive this, accepts the packet and resends the confirmation packet along with its L2 address (MAC address).</p> <p>Note that this broadcast packet contains L1 and L2 data. The L2 contains something called ARP Packet Source which contains the L3 addresses of the source and destination. </p> <p>This broadcast packet is formally called the ARP packet.</p> <p></p>"},{"location":"notes/Routers/#different-network-range-devices","title":"Different network range devices","text":"<p>Now this all works when the devices are in the same network range. Once we try to send frames to a device in another network range, the switch goes haywire.</p> <p>Lets we try to connect a 10.1.1.2 device to a random device in the 23.227.38.0 - 23.227.38.255 range.</p> <p>The 1.2 devices gives up immediately instead of trying to find out the MAC address of the destination device (belonging to the other network range).</p> <p>If we take a closer loop at the ARP packet, the ARP Packet source which should contain the source and destination IP addr. actually contains the source and the gateway IP address. (here gateway is 10.1.1.1)</p> <p></p> <p>Example of a device having which has a gateway address set</p> <p></p> <p>What is the gateway IP address? This address is what is actually used by a router to allow us to connect with devices out of our network range.</p> <p>Which is why we require routers to connect to devices out of our network range.</p>"},{"location":"notes/Routers/#formal-definition-of-routers","title":"Formal definition of routers","text":"<p>Routers are a medium to connect different networks to allow transmission of data between devices that belong to different network (network ranges).</p>"},{"location":"notes/Routers/#what-happens-when-routers-get-a-arp-packet","title":"What happens when routers get a ARP packet?","text":"<p>Recall how the 10.1.1.2 sends an ARP packet to find out the MAC address of a device it wants to connect in 23.227.38.0 - 23.227.38.255 range.</p> <p>Now that the router has received this ARP packet, it further will send another ARP packet as the router itself doesn't have the MAC address of the destination device. Here it sends it the ARP packet to a switch which falls in the network range of the destination device.</p> <p>Now once this ARP packet reaches the switch, the switch then again sends an ARP packet to all the devices connected to it. Once it the destination device receives the ARP packet, it sends back a confirmation packet back to through the same route towards the source.</p> <p>Once the source receives this, it keeps a record of it for future communications.</p>"},{"location":"notes/Subnet%20Mask/","title":"Subnet Mask","text":""},{"location":"notes/Subnet%20Mask/#importance-of-the-subnet-mask","title":"Importance of the Subnet Mask","text":"<p>A subnet mask is important because it defines how many networks and hosts can exist within a given network. By converting an IP address and its subnet mask into binary, we can determine the network bits and host bits.</p> <ul> <li>Network Bits: These bits define the network portion of the IP address and generally remain constant.</li> <li>Host Bits: These bits identify individual hosts within the network and are used to assign addresses to devices.</li> </ul> <p>Number of hosts = 2 ^ (number of host bits) - 2</p> <p>The subtraction of 2 accounts for the network address and the broadcast address.</p> <p>Classless Inter-Domain Routing (CIDR): CIDR extends the number of available IP addresses and provides a compact representation of an IP address and its associated network mask.</p>"},{"location":"notes/Subnet%20Mask/#subnetting-based-on-number-of-networks","title":"Subnetting Based on Number of Networks","text":"<p>If you need to create more networks, you can increase the number of network bits by borrowing bits from the host portion. This process is known as subnetting.</p> <p>To determine how many bits to borrow:</p> <ol> <li>Use powers of 2:<ul> <li><code>128 &lt;- 64 &lt;- 32 &lt;- 16 &lt;- 8 &lt;- 4 &lt;- 2 &lt;- 1</code></li> </ul> </li> <li>For example, if you need 4 additional networks, you need to borrow 2 bits (from 2 -&gt; 4 &amp; also 2^2 = 4).</li> </ol> <p>In CIDR notation, borrowing 2 bits changes the subnet mask from <code>/24</code> to <code>/26</code>:</p> <ul> <li>Original subnet mask: <code>11111111.11111111.11111111.00000000</code> -&gt; <code>/24</code></li> <li>After borrowing 2 bits: <code>11111111.11111111.11111111.11000000</code> -&gt; <code>/26</code></li> </ul> <p>To determine the range of each subnet, find the increment, which is the value of the last network bit used. For a <code>/26</code> subnet mask, the increment is 64.</p> <p>Example subnets:</p> <ol> <li><code>192.168.1.0 - 192.168.1.63</code></li> <li><code>192.168.1.64 - 192.168.1.127</code></li> <li><code>192.168.1.128 - 192.168.1.191</code></li> <li><code>192.168.1.192 - 192.168.1.255</code></li> </ol> <p>To summarise, 1. Calculate the number of host bits required. 2. Borrow the required number of host bits. 3. Find the increment. 4. Create your subnets using the increment.</p>"},{"location":"notes/Subnet%20Mask/#subnetting-based-on-number-of-hosts","title":"Subnetting Based on Number of Hosts","text":"<p>If you need to divide the network based on the number of hosts, you need to keep the host bits and save them accordingly.</p> <p>To determine how many bits to save:</p> <ol> <li>Use powers of 2 to find the number of bits needed:<ul> <li><code>128 &lt;- 64 &lt;- 32 &lt;- 16 &lt;- 8 &lt;- 4 &lt;- 2 &lt;- 1</code></li> <li>By doubling these values: <code>256 &lt;- 128 &lt;- 64 &lt;- 32 &lt;- 16 &lt;- 8 &lt;- 4 &lt;- 2</code></li> <li>For 40 hosts, you need 6 bits (from 2 -&gt; 64 &amp; also 2^6 = 64).</li> </ul> </li> <li>In CIDR notation,<ul> <li>Original subnet mask: <code>11111111.11111111.11111111.00000000</code> -&gt; <code>/24</code></li> <li>After saving bits: <code>11111111.11111111.11111111.11000000</code> -&gt; <code>/26</code></li> </ul> </li> </ol> <p>To determine the range of each subnet, use the increment, which is the value of the last network bit used. For a <code>/26</code> subnet mask, the increment is 64.</p> <p>Example subnets:</p> <ol> <li><code>192.168.1.0 - 192.168.1.63</code></li> <li><code>192.168.1.64 - 192.168.1.127</code></li> <li><code>192.168.1.128 - 192.168.1.191</code></li> <li><code>192.168.1.192 - 192.168.1.255</code></li> </ol> <p>So we would provide the network info to our network admin as such, 1. <code>192.168.1.0 /26</code> 2. <code>192.168.1.64 /26</code> 3. <code>192.168.1.128 /26</code></p> <p>To summarize, 1. Calculate the number of host bits required. 2. Save the required number of host bits. 3. Find the increment. 4. Create your subnets using the increment.</p>"},{"location":"notes/Subnet%20Mask/#reverse-subnetting","title":"Reverse Subnetting","text":"<p>Using reverse subnetting we can find, 1. Network address 2. Broadcast Address 3. Network Range</p> <p>Given, - IPv4 addr: <code>10.200.56.120</code> - Subnet Mask: <code>255.255.254.0</code> - Default Gateway: <code>10.200.56.1</code></p> <p>First lets convert out subnet mask into binary, <code>11111111.11111111.11111110.0</code>, Hence the CIDR notation will be <code>/23</code>.</p> <p>The last network bit here will help us find our increment, which in this case is 2.</p> <p>Now our network starting address will look something like this, <code>10.200.0.0</code></p> <p>Our overall network ranges will look something like this 1. <code>10.200.0.0 - 10.200.1.255</code> 2. <code>10.200.2.0 - 10.200.3.255</code> 3. <code>10.200.4.0 - 10.200.5.255</code> and so on...</p> <p>So Our given IPv4 address has these features, 1. Network Range: <code>10.200.56.0 - 10.200.57.255</code> 2. Network address: <code>10.200.56.0</code> 3. Broadcast Address: <code>10.200.57.255</code></p>"},{"location":"notes/Subnet%20Mask/#other-way-to-reverse","title":"Other way to reverse","text":""},{"location":"notes/Subnet%20Mask/#given","title":"Given:","text":"<ul> <li>IPv4 Address: <code>10.200.56.120</code></li> <li>Subnet Mask: <code>255.255.254.0</code> (which corresponds to <code>/23</code> in CIDR notation)</li> <li>Default Gateway: <code>10.200.56.1</code></li> </ul>"},{"location":"notes/Subnet%20Mask/#step-by-step-explanation","title":"Step-by-Step Explanation:","text":"<ol> <li>Subnet Mask in Binary:</li> <li><code>255.255.254.0</code> in binary is <code>11111111.11111111.11111110.00000000</code>.</li> <li> <p>CIDR notation is <code>/23</code>, indicating that the first 23 bits are used for the network, and the remaining 9 bits are for hosts.</p> </li> <li> <p>Network Address Calculation:</p> </li> <li>The network address is found by ANDing the IP address with the subnet mask.</li> <li><code>10.200.56.120</code> in binary: <code>00001010.11001000.00111000.01111000</code></li> <li>Subnet mask: <code>11111111.11111111.11111110.00000000</code></li> <li>Network Address (AND operation):      <code>00001010.11001000.00111000.01111000      AND      11111111.11111111.11111110.00000000      -----------------------------------      00001010.11001000.00111000.00000000</code></li> <li> <p>The network address is <code>10.200.56.0</code>.</p> </li> <li> <p>Broadcast Address Calculation:</p> </li> <li>The broadcast address is found by setting all the host bits (the remaining bits after the network bits) to <code>1</code>.</li> <li>Network address: <code>00001010.11001000.00111000.00000000</code></li> <li>Broadcast Address:      <code>00001010.11001000.00111001.11111111</code></li> <li> <p>The broadcast address is <code>10.200.57.255</code>.</p> </li> <li> <p>Network Range:</p> </li> <li>The range of IP addresses in this subnet goes from the network address to the broadcast address.</li> <li>Network Range: <code>10.200.56.0</code> to <code>10.200.57.255</code></li> </ol>"},{"location":"notes/Subnet%20Mask/#summary-of-findings","title":"Summary of Findings:","text":"<ul> <li>Network Address: <code>10.200.56.0</code></li> <li>Broadcast Address: <code>10.200.57.255</code></li> <li>Network Range: <code>10.200.56.0</code> to <code>10.200.57.255</code></li> </ul>"},{"location":"notes/Subnet%20Mask/#more-questions","title":"More questions","text":"<p>Here are two application-based questions on subnetting, along with their answers:</p>"},{"location":"notes/Subnet%20Mask/#question-1","title":"Question 1","text":"<p>An organization has been assigned the IP address 192.168.1.0/24 and needs to create 4 subnets to accommodate at least 30 hosts in each subnet. What subnet mask should the organization use, and how many usable hosts will each subnet have?</p> <p>Answer: To create 4 subnets, we need to borrow bits from the host portion of the address. </p> <ol> <li>Calculate the number of bits needed for subnets:</li> <li>$$2^n \\geq 4$$ (where $$n$$ is the number of bits borrowed)</li> <li> <p>Therefore, $$n = 2$$ (since $$2^2 = 4$$)</p> </li> <li> <p>New subnet mask:</p> </li> <li>Original mask is /24 (255.255.255.0).</li> <li> <p>Borrowing 2 bits gives us a new mask of /26 (255.255.255.192).</p> </li> <li> <p>Calculate usable hosts:</p> </li> <li>Remaining bits for hosts = $$32 - 26 = 6$$</li> <li>Usable hosts per subnet = $$2^6 - 2 = 62$$ (subtracting 2 for network and broadcast addresses).</li> </ol> <p>Thus, the organization should use a subnet mask of 255.255.255.192 and will have 62 usable hosts in each subnet.</p>"},{"location":"notes/Subnet%20Mask/#question-2","title":"Question 2","text":"<p>A company has been allocated the IP address range of 10.0.0.0/8 and wants to create subnets for different departments: HR, Sales, and IT. If each department requires at least 50 hosts, what is the appropriate subnet mask for each department, and how many subnets can be created?</p> <p>Answer: 1. Calculate the number of bits needed for hosts:    - For at least 50 hosts, we need enough bits such that $$2^n \\geq 50$$    - The smallest $$n$$ that satisfies this is $$6$$ (since $$2^6 = 64$$)</p> <ol> <li>New subnet mask:</li> <li>Original mask is /8.</li> <li>We need to borrow $$6$$ bits for hosts, leaving us with $$32 - (8 + 6) = 18$$ bits for subnets.</li> <li> <p>Therefore, the new subnet mask is /26 (255.255.255.192).</p> </li> <li> <p>Calculate the number of subnets:</p> </li> <li>The number of borrowed bits for subnets is $$18$$</li> <li>The number of possible subnets = $$2^{(18-8)} = 2^{10} = 1024$$</li> </ol> <p>Thus, the appropriate subnet mask for each department is 255.255.255.192, and the company can create 1024 subnets from the allocated IP address range.</p> <p>Citations: [1] https://www.youtube.com/watch?v=n9twjWSnhgE [2] https://byjusexamprep.com/gate-cse/subnetting-question [3] https://www.flackbox.com/subnetting-practice-questions [4] https://www.subnetting.net/Subnetting.aspx?mode=practice [5] https://www.geeksforgeeks.org/introduction-to-subnetting/ [6] https://learn.microsoft.com/en-us/troubleshoot/windows-client/networking/tcpip-addressing-and-subnetting [7] https://www.youtube.com/watch?v=5AXVLO4VW4U [8] https://www-users.cse.umn.edu/~tripathi/Internet-Examples/HTTP-Key-Differences.pdf</p>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/","title":"Subnetting in IP Networks","text":""},{"location":"notes/Subnetting%20%28GPT%20notes%29/#overview","title":"Overview","text":"<p>Subnetting is a technique used in IP networking to divide a larger network into smaller, more manageable sub-networks (subnets). This allows for better organization, improved security, and efficient use of IP addresses. IP addresses are divided into classes: A, B, and C. Each class has a default subnet mask and a different number of networks and hosts.</p>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#ip-address-classes","title":"IP Address Classes","text":""},{"location":"notes/Subnetting%20%28GPT%20notes%29/#class-a","title":"Class A","text":"<ul> <li>Range: 1.0.0.0 to 126.0.0.0</li> <li>Default Subnet Mask: 255.0.0.0 (/8)</li> <li>Number of Networks: 128 (2^7, minus 2 reserved addresses)</li> <li>Hosts per Network: 16,777,214 (2^24 - 2)</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#class-b","title":"Class B","text":"<ul> <li>Range: 128.0.0.0 to 191.255.0.0</li> <li>Default Subnet Mask: 255.255.0.0 (/16)</li> <li>Number of Networks: 16,384 (2^14)</li> <li>Hosts per Network: 65,534 (2^16 - 2)</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#class-c","title":"Class C","text":"<ul> <li>Range: 192.0.0.0 to 223.255.255.0</li> <li>Default Subnet Mask: 255.255.255.0 (/24)</li> <li>Number of Networks: 2,097,152 (2^21)</li> <li>Hosts per Network: 254 (2^8 - 2)</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#subnetting-class-c-networks","title":"Subnetting Class C Networks","text":""},{"location":"notes/Subnetting%20%28GPT%20notes%29/#key-concepts","title":"Key Concepts","text":"<ol> <li>IP Address and Subnet Mask:</li> <li>An IP address consists of two parts: the network portion and the host portion.</li> <li>The subnet mask determines which part of the IP address is the network portion and which part is the host portion.</li> <li> <p>For a Class C network, the default subnet mask is 255.255.255.0 or /24 in CIDR notation.</p> </li> <li> <p>CIDR Notation:</p> </li> <li>Classless Inter-Domain Routing (CIDR) notation is a shorthand for denoting the subnet mask.</li> <li> <p>/24 means that the first 24 bits of the IP address are the network portion, leaving 8 bits for hosts.</p> </li> <li> <p>Subnetting:</p> </li> <li>Subnetting involves borrowing bits from the host portion to create additional network bits, thus creating more subnets.</li> <li> <p>For example, changing from /24 to /25 (borrowing 1 bit) doubles the number of subnets but halves the number of hosts per subnet.</p> </li> <li> <p>Number of Subnets and Hosts:</p> </li> <li>The number of subnets created can be calculated as 2^n, where n is the number of bits borrowed.</li> <li> <p>The number of hosts per subnet can be calculated as 2^(32 - subnet mask) - 2. The subtraction of 2 accounts for the network and broadcast addresses which cannot be assigned to hosts.</p> </li> <li> <p>Network ID and Broadcast Address:</p> </li> <li>The network ID is the first address in a subnet, used to identify the subnet.</li> <li>The broadcast address is the last address in a subnet, used to send data to all hosts in the subnet.</li> <li>Usable IP addresses are those between the network ID and the broadcast address.</li> </ol>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#example","title":"Example","text":"<p>For a Class C network 192.168.1.0 with a subnet mask of 255.255.255.192 (/26):    - Number of subnets: 2^(26-24) = 4 subnets.    - Number of hosts per subnet: 2^(32-26) - 2 = 62 hosts.</p> <p>The four subnets will be: 1. 192.168.1.0/26    - Network ID: 192.168.1.0    - Broadcast Address: 192.168.1.63    - Usable IP Range: 192.168.1.1 - 192.168.1.62</p> <ol> <li>192.168.1.64/26</li> <li>Network ID: 192.168.1.64</li> <li>Broadcast Address: 192.168.1.127</li> <li> <p>Usable IP Range: 192.168.1.65 - 192.168.1.126</p> </li> <li> <p>192.168.1.128/26</p> </li> <li>Network ID: 192.168.1.128</li> <li>Broadcast Address: 192.168.1.191</li> <li> <p>Usable IP Range: 192.168.1.129 - 192.168.1.190</p> </li> <li> <p>192.168.1.192/26</p> </li> <li>Network ID: 192.168.1.192</li> <li>Broadcast Address: 192.168.1.255</li> <li>Usable IP Range: 192.168.1.193 - 192.168.1.254</li> </ol>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#detailed-calculation","title":"Detailed Calculation:","text":"<ol> <li>Subnet Mask and CIDR Notation:</li> <li>The subnet mask 255.255.255.192 corresponds to /26 in CIDR notation.</li> <li> <p>This means that the first 26 bits are for the network, and the remaining 6 bits are for hosts.</p> </li> <li> <p>Number of Subnets:</p> </li> <li>We are borrowing 2 bits (26 - 24) from the host portion of the default Class C subnet mask (255.255.255.0).</li> <li> <p>Number of subnets = 2^2 = 4 subnets.</p> </li> <li> <p>Number of Hosts per Subnet:</p> </li> <li>Number of hosts per subnet = 2^(32 - 26) - 2 = 2^6 - 2 = 64 - 2 = 62 hosts.</li> <li> <p>The subtraction of 2 accounts for the network address and the broadcast address, which cannot be assigned to hosts.</p> </li> <li> <p>Subnet Increment:</p> </li> <li>Subnet increment = 256 / number of subnets = 256 / 4 = 64.</li> <li>This means each subnet increases by 64 addresses.</li> </ol>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#calculation-of-each-subnet","title":"Calculation of Each Subnet:","text":""},{"location":"notes/Subnetting%20%28GPT%20notes%29/#subnet-1-1921681026","title":"Subnet 1: 192.168.1.0/26","text":"<ul> <li>Network ID: The first address in the subnet range is 192.168.1.0.</li> <li>Broadcast Address: The last address in the subnet range is 192.168.1.63 (192.168.1.0 + 63).</li> <li>Usable IP Range: The addresses between the network ID and the broadcast address are 192.168.1.1 to 192.168.1.62.</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#subnet-2-19216816426","title":"Subnet 2: 192.168.1.64/26","text":"<ul> <li>Network ID: The next subnet starts at 192.168.1.64 (192.168.1.0 + 64).</li> <li>Broadcast Address: The last address in this subnet is 192.168.1.127 (192.168.1.64 + 63).</li> <li>Usable IP Range: The addresses between the network ID and the broadcast address are 192.168.1.65 to 192.168.1.126.</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#subnet-3-192168112826","title":"Subnet 3: 192.168.1.128/26","text":"<ul> <li>Network ID: The next subnet starts at 192.168.1.128 (192.168.1.64 + 64).</li> <li>Broadcast Address: The last address in this subnet is 192.168.1.191 (192.168.1.128 + 63).</li> <li>Usable IP Range: The addresses between the network ID and the broadcast address are 192.168.1.129 to 192.168.1.190.</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#subnet-4-192168119226","title":"Subnet 4: 192.168.1.192/26","text":"<ul> <li>Network ID: The next subnet starts at 192.168.1.192 (192.168.1.128 + 64).</li> <li>Broadcast Address: The last address in this subnet is 192.168.1.255 (192.168.1.192 + 63).</li> <li>Usable IP Range: The addresses between the network ID and the broadcast address are 192.168.1.193 to 192.168.1.254.</li> </ul>"},{"location":"notes/Subnetting%20%28GPT%20notes%29/#why-is-it-256","title":"Why is it 256?","text":"<ol> <li>Total Number of Addresses in One Octet:</li> <li> <p>The fourth octet can have values from 0 to 255, giving a total of 256 values.</p> </li> <li> <p>Number of Subnets:</p> </li> <li>The number of subnets is determined by the number of bits borrowed from the host portion of the address.</li> <li> <p>For example, if you borrow 2 bits for subnetting in a Class C network, you get (2^2 = 4) subnets.</p> </li> <li> <p>Subnet Increment:</p> </li> <li>The subnet increment is calculated as:       [      \\text{Subnet Increment} = \\frac{256}{\\text{Number of Subnets}}      ]</li> <li>In this case, with 4 subnets, the calculation is:      [      \\text{Subnet Increment} = \\frac{256}{4} = 64      ]</li> </ol> <p>By following these steps, you can divide a Class C network into multiple subnets and determine the network ID, broadcast address, and usable IP range for each subnet.</p>"},{"location":"notes/Switches/","title":"Switches","text":"<p>A switch is a device that allows multiple users to connect at once to allow sharing of data through the use of ports.</p> <p>It supports, - Unicast - Multicast - Broadcast</p> <p>The switch works in the layer 2 of the OSI model of network, where it uses the MAC address to allow data transfer between devices.</p>"},{"location":"notes/Switches/#switch-vs-hub","title":"Switch v/s Hub","text":"<p>In switches, we know that the message is only sent to the respective receiver, this wasn't always the case. Earlier Hubs were used, which not only sent the message to the appropriate receiver, but sent the message to every device connected to its ports.</p> <p>This was a major disadvantage. The main reason for this is that the Hub only operated in layer 1 of the OSI model, meaning it didn't have access to the MAC addresses of the devices connected to it. (MAC addresses belong to layer 2).</p>"},{"location":"notes/Switches/#cam-tables","title":"CAM Tables","text":"<p>Content Addressable Memory table, or commonly known as CAM tables, are specially type of memory tables used by switches.</p> <p>This table stores the MAC addresses of the devices connected to its ports. This makes it easy for the switch to transfer messages only to the devices in question.</p> <p>The switches don't automatically have this table generated, the table is built by recording the source address and inbound port of all frames.\u00a0As frames arrive on switch ports, the source MAC addresses are learned and recorded in the CAM table.</p> <p>When a frame arrives at the switch with a destination MAC address of an entry in the CAM table, the frame is forwarded out through only the port that is associated with that specific MAC address. The information a switch uses to perform a lookup in a CAM table is called a key.</p> <p>An example of a network which uses a Switch</p> <p></p> <p>Example of the CAM Table or MAC Forward Table</p> <p></p>"},{"location":"notes/Transport%20layer/","title":"Transport layer","text":"<p>pTransport Layer protocols provide a logical communication between application processes running b/w hosts. This logical communication allows processes to send messages to each other.</p> <p>The TL protocols are implemented on end systems. When the process wants to send message to another process, the message will pass through its application layer to its transport layer. Here the transport layer will split this message into chunks and attach a transport layer header, these chunks are called segments.</p> <p>The segments are sent down to the network layer where they are encapsulated with the network layer header. (datagram)</p> <p>Transport layer provides logical communication between processes in end systems whereas network layer provides logical communication b/w hosts.</p> <p>Service provided an upper layer usually depends on a service provided by a lower layer.</p> <p>Transport layer packets are referred to as segments. Particularly, packets sent using TCP are segments and using UDP are datagrams.</p> <p>Since network layer packets are also datagrams, we refer to TCP/UDP packets as segments.</p> <p>Network layer has its own protocol, Internet Protocol. It's called a best effort delivery service protocol, i.e. it tries its best to deliver the packets but it doesn't guarantee the delivery of the packets. So we can call the IP a unreliable protocol.</p> <p>The IP is aided with the UDP/TCP to make it more reliable. TL protocols extend their help to aid IP to help with its delivery service.</p> <p>Although, IP aided with UDP is actually more unreliable compared to IP aided with TCP as TCP provides reliable data transfer and congestion control. </p> <p>Congestion control allows equal share of bandwidth to TCP connections. This protocol prevents one connection from swamping the links &amp; routers </p> <p>Extending host-to-host delivery to process-to-process is called transport layer multiplexing and demultiplexing.</p>"},{"location":"notes/Transport%20layer/#multiplexing-demultiplexing","title":"Multiplexing &amp; Demultiplexing","text":"<p>The host-to-host delivery service provided by the network layer extended to a process-to-process delivery service for applications running on hosts. This service is required by every computer running applications.</p> <p>A process can have one or more sockets at the same time, hence the sockets require a unique identifier which is the port number. But instead of sending the message directly to the socket at its port number, the msg is first sent to an intermediate port.</p> <p>At the receiving end, the transport layers receives the segments and examines it for the destination port number and sends the message accordingly. This is demultiplexing.</p> <p>The job of gathering data, port numbers (source and destination) &amp; attaching the transport layer header to the segments and then sending this to the network layer is called multiplexing ![[Pasted image 20240830100108.png]]</p> <p>Hence Multiplexing requires, 1. Unique identifier for ports 2. Each segment having special fields indicating the destination socket.</p> <p></p> <p>Port number is 16 bit number ranging from 0 to 65535. Well known ports range from 0 to 1023. 1024 to 49151 are Registered ports. 49152 - 65535 are dynamic/private ports.</p>"},{"location":"notes/Transport%20layer/#connectionless-mux-and-demux","title":"Connectionless mux and demux","text":"<ul> <li>UDP is used this.</li> <li>During UDP socket programming, OS automatically assigns ports numbers to sockets or the application programmer can assign it their selves.</li> <li>If the programmer is implementing a well known port program, they must assign the socket the well known port itself.</li> <li>UDP socket is fully identified by a two tuple, dest. IP addr and dest. port number</li> <li>If two UDP segments have different source IP addr and/or port numbers but same dest. ip &amp; port number, then the two segments are directed to the same process via the same socket.</li> </ul> <p>Example</p> <p>Host A (UDP port 19157) wants to send to -&gt; Host B (UDP port 46428)</p> <ul> <li>A creates segment which includes =&gt; Dest. port, source port, and other values</li> <li>segment -&gt; n/w layer -&gt; encapsulated w/ n/w header -&gt; datagram</li> <li>A datagram sent to B. </li> <li>B transport layer receives and examines segment.</li> <li>Sends message to desired port (46428)</li> </ul>"},{"location":"notes/Transport%20layer/#connection-oriented-mux-and-demux","title":"Connection oriented mux and demux","text":"<ul> <li>TCP socket is identified by a four tuple, source IP, source Port, dest. IP, dest Port.</li> <li>Two TCP segments with different source IP and/or port numbers are directed to two different sockets.</li> <li>In a simple TCP implementation application, the server side has a \"welcome socket\" ready to receive connection requests. The TCP client sends a connection establishment request.</li> </ul> <p>Connection establishment request contains nothing but the dest. port number and some special connection establishment bit set in TCP header and source port number.</p>"},{"location":"notes/Transport%20layer/#port-security","title":"Port security","text":"<p>A process on the server usually waits for requests on an open port. Once the client finds this open port it can easily map out the process linked to it. </p> <p>This can be useful in finding which network applications are running on the host. But this can also lead to a security vulnerability, if the open port has a known security flaw, then the host is doomed for an attack.</p>"},{"location":"notes/Transport%20layer/#web-server-tcp","title":"Web server &amp; TCP","text":"<p>Earlier web servers created a new process which had its own socket for every new TCP connection made. But new high-performing web servers use only one connection. </p> <p>Servers create threads for each new connection with a new socket for each connection. So there could be at a given moment there could multiple sockets pointing to the same process.</p> <p>In a persistent connection, one socket is responsible for HTTP communication between client and server.</p> <p>In non-persistent connection, multiple sockets are created and closed for each request &amp; response between the client and server. This can create load on a busy server.</p>"},{"location":"notes/Transport%20layer/#connectionless-transport-udp","title":"Connectionless Transport: UDP","text":"<ul> <li>UDP is a connectionless protocol with little error checking and mux &amp; demux functions.</li> <li>UDP adds almost nothing to IP.</li> </ul> <p>We know,  - UDP takes the message from application process, attaches source &amp; dest port number for mux &amp; demuxing and other small fields. - This segment is encapsulated with the transport layer header and sent to the network layer. Then it's further encapsulated with the network header to form the IP datagram. This datagram is then sent to the destination. - As we see, there is no handshaking done between the source &amp; destination. Hence UDP is connectionless.</p> <p>And, - DNS is another application layer protocol that uses UDP as its underlying protocol. - The host side UDP makes its segment, passes it to the network layer to make the datagram and this is sent to the name server - If DNS application at the host side doesn't get any reply, it tries sending a query again &amp; if after repeated trying it doesn't get a reply, it informs the invoking application.</p> <p>Why UDP over TCP if it's unreliable? 1. Finer application layer control over when data is sent     - UDP quickly encapsulates the message w/ the headers and port information and sends it to the network layer.      - On the other hand TCP has congestion control &amp; does a three-way handshake. TCP keeps on sending a packet until it receives the ack. packet back from destination. 2. No connection establishment     - UDP just blasts away without any formal preliminaries. UDP doesn't introduce any delays.     - This is probably the reason why DNS using UDP.     - Google Chrome browsers use QUIC, Quick UDP Internet Connection. Which uses UDP as its underlying transport protocol and implements reliability in an application layer protocol on top of UDP. 3. No connection state     - TCP maintains a connection for each request and response whereas UDP doesn't maintain any connection and doesn't track any parameters 4. Small packet header overhead     - UDP has only 8 bytes of header and TCP has 20 bytes of header.</p> <p>But the delays introduced by HTTP over TCP is necessary to download important documents.</p> <p></p> <ul> <li>UDP provides no congestion protocol but this protocol needed in a congested state in which very little useful work is done.</li> <li>When there is a congested state, all the UDP packets wouldn't traverse from source to destination. This would in turn block the TCP packets.</li> <li>Although, reliability is something that can be implemented in the application itself while using UDP as the underlying protocol.</li> <li>QUIC is an example of such thing.</li> </ul>"},{"location":"notes/Transport%20layer/#udp-segment-structure","title":"UDP segment structure","text":"<ul> <li>Application data contains DNS query or response message or application data itself.</li> <li>UDP header has four fields, each of 2 bytes.</li> <li>Length field specifies the number of bytes in the UDP segment.</li> <li>Checksum field tells whether any error has occurred or not. </li> </ul>"},{"location":"notes/Transport%20layer/#checksum","title":"Checksum","text":"<ul> <li>Checksum is a method to check for any errors/if any bits are manipulated in the segment.</li> <li>First addition of the 16 bit words in the segment is done, Any overflow encountered is wrapped around.</li> <li>After getting the sum, 1's complement on the sum is performed. This resulting 16 bits are sent along with the other 16 bit words.</li> <li>The destination receives this segment and adds up these 16 bit words along with the checksum. If the result is just 16 1's, then there is no error. If there is a 0, then there is an error.</li> </ul> <p>But why does UDP need this error detection method? - No guarantee about error checking between source and destination - In router memory could destory bits. - UDP provides error checking on an end to end basis - Although UDP provides this error checking method, it doesn't provide any method to overcome it.</p> <p>OS fundamental end to end principle: Functions provided at the lower level maybe redundant or of little value when compared to the cost of providing them at the higher level.</p>"},{"location":"notes/Transport%20layer/#principles-of-reliable-data-transfer","title":"Principles of reliable data transfer","text":"<p>![[Pasted image 20240901164358.png]]</p> <p>rdt_send() - reliable data transfer function used by senders, invoked from above. udt_send() - unreliable data transfer allows data to be sent to the other side. rdt_recv() - called when data is received by receiving side. deliver_data - data sent to upper layer</p> <ol> <li>First iteration<ul> <li>Sender side<ul> <li>Get data from upper layer (system call to upper layer)</li> <li>Pack data</li> <li>Send data</li> </ul> </li> <li>Receiver side<ul> <li>Get data from lower layer (system call to lower layer)</li> <li>Unpack data</li> <li>Send data to upper layer</li> </ul> </li> <li>![[Pasted image 20240901161206.png]]</li> </ul> </li> <li>Second iteration (stop and wait ARQ)<ul> <li>Sender side<ul> <li>After sending expect a ACK or NACK packet</li> <li>if NACK, send data again</li> </ul> </li> <li>Receivers side<ul> <li>After getting data, send ACK (if not corrupt) or send NACK (if corrupt)</li> <li>wait for data</li> </ul> </li> <li>Retransmission of data is aka Automation Repeat reQuest protocol (ARQ)</li> <li>![[Pasted image 20240901161227.png]]</li> </ul> </li> <li>Second but better iteration (adding sequence number)<ul> <li>Problems: ACK, NACK could be corrupted</li> <li>Solution: Data packets are included with a sequence number. Note that ACK &amp; NACK need not include this number.</li> <li>Senders side<ul> <li>Send data but now with a sequence number and wait for ACK, NACK</li> <li>if NACK/ACK not received on time, send data again, else move to next sequence number</li> <li>![[Pasted image 20240901162937.png]]</li> </ul> </li> <li>Receivers Side<ul> <li>Get the packet and check if sequence is as expected, send an ACK otherwise also send an ACK</li> <li>If packet is corrupted, send a NACK with the checksum</li> <li>![[Pasted image 20240901162927.png]]</li> </ul> </li> </ul> </li> <li>Second but slightly better iteration (NACK free)<ul> <li>Improvised: Instead of sending NACK, send a ACK for the previous packet, the senders side will understand that the recently sent packet was corrupted.</li> <li>Here, ACK needs to include the sequence number now.</li> <li>Senders Side<ul> <li>after sending packet wait for ACK</li> <li>if dup ACK is received, for prev packet, retransmit current packet.</li> <li>![[Pasted image 20240901163227.png]]</li> </ul> </li> </ul> </li> <li>Third iteration<ul> <li>Problem: Underlying channel can lose packets. We need to detect packet loss and what to do when packet loss occurs.</li> <li>Intuition: We need a mechanism where the sender doesn't wait for the worst-case scenario (round trip delay b/w recv and sender + amt of time to process packet at recv) but instead waits for a selected amt of time.</li> <li>Solution: Since sender doesn't know whether packet/ack was lost or overdelayed, the sender simply retransmits the packet. Hence we can implement a count-down timer where in the sender can start it, respond appropriately to any timer interrupts &amp; stop it.</li> <li>Since packet numbers alternate, this iteration/protocol is aka alternating bit protocol.</li> <li>Senders Side<ul> <li>![[Pasted image 20240901165343.png]]</li> </ul> </li> <li>![[Pasted image 20240903103608.png]]</li> </ul> </li> </ol>"},{"location":"notes/Transport%20layer/#pipelined-reliable-data-transfer","title":"Pipelined reliable data transfer","text":"<p>![[Pasted image 20240904093520.png]]</p> <ul> <li>Stop and wait protocol is time consuming even if the bandwidth of the channel is big enough.</li> </ul> <p>Example - RTT is approx 30 ms - Transmission rate, R is 1Gbps (10^9) - Packet size, L is of 1,000 bytes (8000 bits)</p> <p>With stop-and-wait, time to transmit the packet through the link is</p> <pre><code>d_trans = L/R = 8000 bits per packet / 10^9 bits per sec = 8 microseconds\n</code></pre> <p>Say the transmission begins at, t = 0, then at <code>t = L/R</code> the last bit enters the channel at the sender side.</p> <p>Then the receiver receives the last bit at, <code>t = RTT/2 + L/R</code> = 15.008 msec</p> <p>The receiver sends the ACK bit back at, <code>t = RTT + L/R</code> = 30.008 msec</p> <p>If we calculate the time the sender is actually busy sending the bits, (aka utilization of the sender),</p> <pre><code>U_sender = (L/R)/(RTT+L/R) = 0.008 / 30.008  = 0.00027\n</code></pre> <p>i.e. The sender is only able to send 276 KBps in a 1Gbps link.</p> <p>![[Pasted image 20240904094847.png]]</p> <p>Solution to this problem is simple. The sender is allowed to send multiple packets at once rather than waiting for acknowledgement for each packet. This technique is know as pipelining.</p> <p>Consequences of pipelining, 1. Must increase range of sequence numbers. 2. The hosts at both ends must buffer more packets.</p> <p>![[Pasted image 20240904095239.png]]</p>"},{"location":"notes/Transport%20layer/#go-back-n-gbn","title":"GO-Back-N (GBN)","text":"<p>GBN protocol allows the sender to send packets at once, filling the pipeline, without waiting for acknowledgment from the receiver. The sender can send a max of N packets at once. N is also the window size.</p> <p>GBN is also referred to as the sliding window protocol</p> <p>![[Pasted image 20240904095459.png]]</p> <ul> <li>k no. of bits -&gt; Range of sequence numbers <code>[0, 2^k - 1]</code></li> <li>After the range is over, and there are still packets remaining, the next sequence should start with <code>modulo 2^k - 1</code>.</li> </ul> <p>Events, 1. Invocation from above     - <code>rdt_send()</code> checks if the pipeline is full before sending data to the pipeline. The upper layer tries again if it's full.      - In real implementation, the data is probably buffered 2. Cumulative Acknowledgement     - All packets upto \"n\" are taken to be a cum. ack. indicating that pakcets upto \"n\" have been received. 3. Timeout event     - There is a single timer, which can be thought of as a timer for the oldest packet sent but not acknowledged.     - If the timer runs out, sender resends all the packets that haven't been acknowledged.     - If ACK is received, but there are still packets that aren't acknowledged, the timer resets.</p> <p>![[Pasted image 20240904100508.png]]</p> <p>Here's how it works: 1. Sending Packets: The sender transmits multiple packets within a window size. 2. Receiving ACKs: The receiver sends an ACK for each correctly received packet. If a packet is lost (like pkt2 in the diagram), the receiver will not acknowledge it, and it will continue to acknowledge the last correctly received packet (ACK1 in this case). 3. Handling Duplicate ACKs: When the sender receives multiple ACKs with the same sequence number (indicating that the receiver has not received the next expected packet), the sender understands that there may be a problem, but it does not take any immediate action. 4. Timeout and Retransmission: The sender waits for the timeout to occur. Once the timeout expires, the sender will retransmit the missing packet (pkt2) and all subsequent packets (pkt3, pkt4, pkt5) in the window.</p> <p>In the above case if pkt2 wasn't lost but only got delayed (i.e would reach later but before the timer stops), then pkt3 would have been buffered and not retransmitted. But since pkt2 is lost, and no ACK2 is received within the timeframe, pkt2 is retransmitted.</p> <p>Note that if k packets have been received in-order, then it can be said that all packets upto k seq. number have been received (cumulative acknowledgement).</p>"},{"location":"notes/Transport%20layer/#selective-repeat","title":"Selective Repeat","text":"<p>While GBN allows us to \"fill the pipeline\", it can get inconvenient at times when we are sending a large chunk of data and one of the packets get lost, then we'd have to send a large part of the data again, i.e. retransmission of large data is inconvenient.</p> <p>Selective repeat protocol allows us to send only selective part of the data that can be considered to be lost. Again, a window size of N is maintained here.</p> <p>In SR, the out of order packets are buffered until the correct ordered packets aren't received.</p> <p>![[Pasted image 20240904101922.png]]</p> <p>Sender events, 1. Data received from above, the receiver checks whether the incoming packet fits the senders window size, if not the packet is buffered and transmitted to the upper layer later. If it is, the packet is sent. 2. Timer, each packet has its own timer. 3. ACK received, when a packet is received, the receiver marks the sequence number. If the seq number matches the <code>send_base</code>, then the window is moved forward.</p> <p>Receiver events, 1. For sequence numbers, <code>[rcv_base, rcv_base+N-1]</code>, i.e. the window size, ACK is returned to the sender, if packet wasn't received earlier, it's buffered. 2. For sequence numbers, <code>[rcv_base-N, rcv_base-1]</code>, packet is correct 3. Otherwise packet is incorrect</p> <p>![[Pasted image 20240904104232.png]]</p> <p>In the above diagram, since pkt2 is lost/not received, the next pkts are buffered. Once the timer for the pkt2 is over, ACK2 is sent, and the remaining buffered packets are sent to the upper layer.</p> <p>Due to this, there is no synchronization between the receiver and the sender as to what has been correctly received and what has not.</p> <p>![[Pasted image 20240904104914.png]]</p>"},{"location":"notes/Transport%20layer/#connection-oriented-transport-tcp","title":"Connection oriented transport - TCP","text":"<ul> <li>TCP is connection oriented because before the one end of the TCP connection can send data to the other end of the TCP connection, there must be a handshake between the processes.</li> <li>TCP connection is a logical connection which maintains the state residing only in the TCPs in the communicating end systems</li> <li>Full duplex service</li> <li>Point to point</li> <li>No multicasting support</li> <li>Before establishing a connection, client sends a segment first. The server then responds with a special segment to which the client again responds with a segment (three way handshake). The third segment may carry the payload.</li> </ul> <p>Flow, - Client process passes data through its socket. - After passing through the socket, transfer of data is the responsibility of TCP. - The client TCP will forward it to the connection's send buffer. - Time to time, TCP (client) will collect chunks from the buffer and send it to the network layer. These segments are encapsulated separately to form IP datagrams.  - When TCP receives the segment on the other end, the segment is stored in the receiver's buffer</p> <p>RFC 793 specifies that TCP should segments of data at its own rate/convenience.</p> <ul> <li>The max amount of data that can be grabbed and sent in a segment is called the Maximum segment size (MSS).</li> <li>MSS is decided by checking the length of the largest link layer frame that can be sent by the local host. </li> </ul> <p>![[Pasted image 20240910195053.png]]</p>"},{"location":"notes/Transport%20layer/#tcp-segment-structure","title":"TCP segment structure","text":"<p>![[Pasted image 20240910201308.png]]</p> <p>CWR and ECE are as explicit congestion bits.</p>"},{"location":"notes/Transport%20layer/#seq-num-and-ack-number","title":"Seq num and ack number","text":"<p>In TCP, data is viewed as unstructured but ordered stream of bytes.</p> <ul> <li>critical part of TCP's reliable data transfer</li> <li>Sequence number number for a segment is the byte-stream number of first byte in the segment.</li> <li>Each sequence number is inserted in the sequence number field in the header of the appropriate TCP segment. As shown in the figure, the first byte, of the data stream, is numbered 0. Then second segment is 1000, &amp; so on.</li> <li>Acknowledgment in TCP is complex. The ack number that host A puts in the segment, is the sequence number of the next byte Host A expects from host B. </li> <li>Say that Host A sends the 1st segment with an ack number of 0, then the host B will send a response with ack number as 1000. Then Host A will send the next segment.</li> <li>Specifically, it tells the sender, \"I have received everything up to byte number X, and I expect the next byte to be X+1.\"</li> <li>Now say that Host A has sent two segments at one after the other, but the Host B has sent ack for the second segment and not the first one. Host A will then resend the first segment with the same ack number.</li> <li>TCP is said to provide cumulative acknowledgment.</li> </ul> <p>If the segments have arrived out-of-order, TCP on its own doesn't need to do anything, it's up to the programmer on how they program it.</p> <p>![[Pasted image 20240920182601.png]]</p>"},{"location":"notes/Transport%20layer/#telnet-case-study","title":"telnet case study","text":"<ul> <li>application layer protocol</li> <li>designed to work b/w any pair of hosts</li> <li>data sent using telnet is not encrypted.</li> <li>![[Pasted image 20240920184714.png]]</li> <li>The first segment carries the starting sequence number (assumed for client as 42) and the acknowledgement number (assumed for server as 79).</li> <li>Once the client receives this segment, it responds back with the seq number as 79 and ack number as 43 (stating it has received data upto 43 seq number).</li> <li>Then the host sends back another confirmation with the seq number incremented and ack number as 80, indicating it's expecting confirmation for data with seq number upto 80.</li> <li>Purpose of second segment<ol> <li>acknowledgment that the data is received</li> <li>echo back the letter 'C'</li> </ol> </li> <li>Purpose of third segment<ol> <li>ack that data from server is received.</li> </ol> </li> </ul> <p>The ack for client-to-server data is carried in a segment carrying server-to-client data, this ack is piggybacked  on the server to client data segment</p>"},{"location":"notes/Transport%20layer/#estimating-round-trip-time","title":"estimating round trip time","text":"<ul> <li><code>SampleRTT</code> - amount of time passed from when the segment was sent and the acknowledgment for that segment is received.</li> <li>SampleRTT is only measured for one segment that is currently acknowledged. It's also not calculated for retransmitted segments.</li> <li>It fluctuates from segment to segment due to congestion protocol, hence we need some average value.</li> <li><code>EstimatedRTT</code> - average value of the previous SampleRTT's.</li> </ul> <pre><code>EstimatedRTT = (1-alpha)EstRTT - alpha(SampleRTT)\nwhere alpha = 0.125\n</code></pre> <ul> <li>ERTT takes the weighted average of SRTT where the most recent SRTT's have more weightage. This type of average called an exponential weighted moving average.</li> <li>It's also necessary to know how much SRTT deviates form ERTT.</li> <li><code>DevRTT</code> - measures the deviation between SRTT &amp; ERTT.</li> </ul> <pre><code>DevRTT = (1-beta)DevRTT * beta (|SRTT - ERTT|)\nwhere beta = 0.25\n</code></pre> <ul> <li>DRTT is also a EWMA of the difference b/w SRTT &amp; ERTT, where little fluctuation in SRTT results in little fluctuation in DRTT, and high fluctuation in SRTT results in high fluctuation in DRTT.</li> <li>Now the timeout interval, it should be equal to ERTT plus some margin.</li> <li><code>Timeout Interval</code> - time the client waits for an ack of a sent segment before assuming the seg was lost or delayed.</li> </ul> <pre><code>Timeout Interval = ERTT + 4 * DRTT\n</code></pre> <p>When a timeout occurs, the timeout is doubled to avoid premature timeout again. Once the ack is received, ERTT is updated and so the interval is calculated again.</p>"},{"location":"notes/Transport%20layer/#reliable-data-transfer","title":"reliable data transfer","text":"<p>IP provides unreliable data transfer and doesn't guarantee in-order delivery. The bits can get lost, corrupted, shuffled &amp; even overflow the routers.</p> <ul> <li>TCP creates a reliable data transfer service on top of IP.</li> <li>Earlier we kept an individual timer with each transmitted but unack segment. This creates overhead. Hence it's recommended to use a single transmission timer, even if there are multiple unacked segments. ![[Pasted image 20240920193822.png]]</li> </ul> <pre><code>[Application Layer]\n      |\n      v\n[TCP Sender] --- Creates Segment with Sequence Number ---&gt; [IP Layer]\n      |\n      v\n[Timer Starts] (If no other segment\u2019s timer is running)\n[Timer Expires] \n      |\n      v\n[TCP Retransmits Segment] --- Sends the same segment again ---&gt; [IP Layer]\n      |\n      v\n[Timer Restarts]\n[ACK Received] (ACK number: y)\n      |\n      v\n[TCP Checks SendBase]\n      |\n      v\nIF (y &gt; SendBase)\n      |\n      v\n[TCP Updates SendBase to y] --- [SendBase = y] (Acknowledges all bytes before y)\n      |\n      v\n[TCP Restarts Timer] (If there are pending unacknowledged segments)\n</code></pre> <ul> <li>The above scenario is a highly simplified version of TCP. But it also has issues.</li> <li>Cases<ol> <li>A sends segment -&gt; B's ack is lost -&gt; Timeout occurs -&gt; A retransmits -&gt; B sends ack again</li> <li>A sends two segs -&gt; B sends ack for second and a random previous segment -&gt; timeout for A's first seg -&gt; A retransmits first seg but not second.</li> <li>A sends two segs -&gt; B sends ack for first -&gt; ack lost -&gt; but before timeout, B sends ack for second -&gt; second ack tells that everything before second ack is received (i.e. first segment is also received).</li> </ol> </li> </ul>"},{"location":"notes/Transport%20layer/#doubling-time-interval","title":"doubling time interval","text":"<ul> <li>when a packet is retransmitted, the timeout is doubled from the its previous old value and not derived form ERTT &amp; DRTT.</li> <li>this mod provides a limited form of congestion control.</li> <li>if the packets are retransmitted too many times, this might get congested.</li> </ul>"},{"location":"notes/Transport%20layer/#fast-retransmit","title":"fast retransmit","text":"<ul> <li>problem with time interval, they can get too long and cause end-to-end delay. </li> <li>this can be avoided by the sender detecting the packet loss by noting the duplicate ACK's.</li> <li>this dup ack tells the sender that the sender has already received ack for a segment</li> <li>![[Pasted image 20240920195826.png]]</li> <li>sender often sends a chunk of data. if one of the data is lost, the receivers keeps on sending ack's for the lost data. </li> <li>if the dup acks &gt; 3, then the sender understands that the packet is lost and does a fast retransmit. i.e. retransmitting before the that segments timer expires.</li> <li>![[Pasted image 20240920205203.png]]</li> <li>![[Pasted image 20240920205213.png]]</li> <li>![[Pasted image 20240920205223.png]]</li> </ul>"},{"location":"notes/Transport%20layer/#error-recovery-mechanism","title":"error recovery mechanism","text":"<ul> <li>selective acknowledgment allows a TCP to receiver to acknowledge out-of-order segments selectively rather than just cumulatively acknowledge the last correctly received, in-order segment.</li> <li>this is a hybrid of GBN &amp; SR protocol.</li> </ul>"},{"location":"notes/Transport%20layer/#flow-control","title":"flow control","text":"<p>tcp connections on the end systems maintain a receiver buffer from the applications read data one by one. they dont read it immediately when the buffer is being filled.</p> <ul> <li>tcp provides a flow control protocol service that eliminates the buffer from being overflowed by received data. aka a speed matching service.</li> <li><code>Receive window or rwnd</code> - tells the sender about the available space in the receivers buffer. both end systems store this information.</li> <li>The receiver keeps track of a few variables</li> <li><code>LastByteRead</code> - the number of the last byte read in the data stream from the buffer</li> <li><code>LastByteRcvd</code> - number of the last byte received in the data stream that has arrived from network to receive buffer</li> <li>Since TCP doesn't allow overflow</li> </ul> <pre><code>LastByteRcvd - LastByteRead &lt;= RcvBuffer\n</code></pre> <ul> <li>therefor the rwnd is always calculated as the above values change, rwnd is a dynamic value.</li> </ul> <pre><code>rwnd = RcvBuffer - [LastByteRcvd - LastByteRead]\n</code></pre> <ul> <li>The sender keeps track of two variables, <code>LastByteSent</code> &amp; <code>LastByteAcked</code>. There difference is the amount of segments that are unacknowledged and sent by the sender.</li> </ul> <pre><code>LastByteSent - LastBytAcked &lt;= rwnd\n</code></pre> <ul> <li>Since the rwnd can get full or (rwnd = 0) and assuming the receiver doesnt want to send anything either. so when the rwnd != 0, the sender won't know this.</li> <li>So to overcome this, sender must send 1 byte messages to check whether rwnd != 0, once this is true, the byte is acked with the rwnd value.</li> </ul>"},{"location":"notes/Transport%20layer/#tcp-connection-management","title":"tcp connection management","text":"<p>initiation 1. the client side TCP sends a special segment to the server side TCP server.      1. doesn't contian application layer data     2. contains a SYN bit set to 1.     3. aka SYN segment     4. client also choose random initial sequence number     5. isn is packed with the IP datagram and sent to server 2. the server side receives the segment     1. it dedcapsulates it and assigns allocates the TCP buffers and variables.     2. it then sends a connection-granted segment back to client     3. it includes the SYN bit set to 1     4. it also includes a random initial sequence number (diff from client).     5. no application layer data     6. aka SYNACK segment 3. client allocates buffer and variables to connection     1. sends a final ack segment     2. ack is server_isn + 1     3. SYN bit is 0.</p> <p>this is the three-way handshake.</p> <p>termination, 1. client &amp; server deallocate the buffer and variables. 2. the client process asks for termination 3. the tcp client sends a segment w/ the FIN bit set to 1. 4. the tcp server responds with ack, w/ FIN bit set to 1. 5. the tcp server again sends a shutdown segment with FIN bit set to 1, initiating a shutdown. 6. the client sends an ack segment.</p> <p>states visited by client and server ![[Pasted image 20240920215514.png]] ![[Pasted image 20240920215522.png]]</p> <p>NMAP Port Scanning Tool  In our example, NMAP sends a SYN segment to port 6789. - TCP SYNACK from target host to source host means open port - TCP RST from target host to source indicates that the port has no firewall - No response means the SYN segment was blocked by a firewall</p> <p>![[Pasted image 20241126181700.png]]</p>"},{"location":"notes/Transport%20layer/#principles-of-congestion-control","title":"principles of congestion control","text":"<ol> <li>Feedback Mechanism: Use feedback from the network (e.g., packet loss, delays) to adjust the sender's transmission rate dynamically. This helps avoid overwhelming the network.</li> <li>Window-based Control: Implement mechanisms like TCP\u2019s sliding window protocol, where the sender maintains a window of packets that can be sent before needing an acknowledgment. The window size can be adjusted based on network conditions.</li> <li>Congestion Avoidance: Proactively manage traffic by reducing the transmission rate before congestion occurs. This can involve algorithms that gradually increase the sending rate until packet loss is detected.</li> <li>Rate Limiting: Establish maximum sending rates for hosts to prevent excessive data flow into the network, ensuring fair access for all users.</li> <li>Load Balancing: Distribute traffic across multiple paths or links to avoid congestion on any single path. This can involve techniques like multipath routing.</li> <li>Explicit Congestion Notification (ECN): Utilize network signals (like marked packets) to inform senders about impending congestion, allowing them to reduce their sending rates preemptively.</li> <li>Retransmission Strategies: Implement strategies for retransmitting lost packets, considering both reliability and network congestion. Adjust retransmission times based on current network conditions.</li> <li>Fairness: Ensure that all users and applications receive a fair share of the network resources, preventing any single user from monopolizing bandwidth.</li> <li>Adaptive Algorithms: Use adaptive algorithms that can learn from past network behavior, optimizing the sending rate based on changing conditions.</li> <li>Monitoring and Analysis: Continuously monitor network performance metrics (like throughput and delay) to inform congestion control decisions and improve overall network efficiency.</li> </ol>"},{"location":"notes/Transport%20layer/#congestion-control-in-tcp","title":"Congestion Control in TCP","text":"<p>Congestion control is a crucial aspect of the Transmission Control Protocol (TCP) that manages data flow to prevent network congestion. It employs several algorithms and mechanisms to optimize throughput while minimizing packet loss. Key components of TCP congestion control include Slow Start, Congestion Avoidance, Fast Recovery, and others.</p>"},{"location":"notes/Transport%20layer/#slow-start","title":"Slow Start","text":"<p>TCP Slow Start is the initial phase of TCP congestion control. When a TCP connection is established, the sender begins with a small congestion window (cwnd), typically set to one Maximum Segment Size (MSS). The sender gradually increases the cwnd exponentially with each acknowledgment (ACK) received. For instance, if the cwnd starts at 1 MSS, it doubles with every successful ACK until it reaches a predefined threshold known as the slow start threshold (ssthresh). This exponential growth continues until a packet loss is detected or the ssthresh is reached, allowing TCP to quickly probe the network's capacity without overwhelming it.</p>"},{"location":"notes/Transport%20layer/#congestion-avoidance","title":"Congestion Avoidance","text":"<p>Once the cwnd surpasses the ssthresh, TCP transitions to the Congestion Avoidance phase. In this phase, the increase in cwnd becomes linear rather than exponential. The cwnd increases by one MSS for each round-trip time (RTT) of successful transmissions. This approach helps maintain a steady flow of data while avoiding congestion, as it allows for gradual adjustments based on network conditions.</p>"},{"location":"notes/Transport%20layer/#fast-recovery","title":"Fast Recovery","text":"<p>Fast Recovery is an optimization technique used in conjunction with Fast Retransmit. When packet loss is detected via duplicate ACKs (typically three), TCP enters Fast Recovery instead of returning to Slow Start. The ssthresh is set to half of the current cwnd, and the cwnd is adjusted to this new value. This allows for quicker recovery from packet loss while maintaining some level of throughput, as it enables the sender to continue transmitting new data rather than restarting from a low cwnd.</p>"},{"location":"notes/Transport%20layer/#tcp-splitting","title":"TCP Splitting","text":"<p>TCP splitting involves dividing a single TCP connection into multiple parallel connections. This technique can enhance throughput and improve performance over high-bandwidth paths by allowing simultaneous data streams, effectively utilizing available bandwidth and reducing latency.</p>"},{"location":"notes/Transport%20layer/#tcp-reno-and-tcp-tahoe","title":"TCP Reno and TCP Tahoe","text":"<p>TCP Reno and TCP Tahoe are two versions of TCP that implement different strategies for congestion control. </p> <ul> <li>TCP Tahoe: Utilizes Slow Start and Congestion Avoidance but returns to Slow Start after detecting packet loss.</li> <li>TCP Reno: Introduces Fast Recovery, allowing it to recover from packet loss without reverting to Slow Start, thus maintaining higher throughput. Here\u2019s a comparison of how TCP Tahoe and TCP Reno react to packet loss, specifically focusing on the scenarios of timeout and receiving three duplicate ACKs:</li> </ul> Feature TCP Tahoe TCP Reno Response to Timeout Upon detecting a timeout, Tahoe reduces the congestion window (cwnd) to 1 MSS and sets the slow start threshold (ssthresh) to half of the current cwnd. It then enters the Slow Start phase to probe the network again. Similar to Tahoe, upon a timeout, Reno also resets cwnd to 1 MSS and sets ssthresh to half of the current cwnd. It then restarts in Slow Start. Response to 3 Duplicate ACKs When three duplicate ACKs are received, Tahoe performs a Fast Retransmit, sets ssthresh to half of the current cwnd, and reduces cwnd to 1 MSS. It then enters Slow Start again, effectively restarting the congestion control process. Upon receiving three duplicate ACKs, Reno performs Fast Retransmit but instead of reducing cwnd to 1 MSS, it halves the cwnd and sets ssthresh equal to this new value. Reno then enters Fast Recovery, allowing it to continue sending new data without returning to Slow Start immediately. Performance During Recovery Tahoe's approach can lead to slower recovery from packet loss since it resets cwnd and starts over from a low value, which can significantly reduce throughput in congested networks. Reno's Fast Recovery allows it to maintain a higher throughput during recovery since it does not reset cwnd completely but rather halves it, enabling quicker adaptation to network conditions. Efficiency Less efficient in high-loss environments due to frequent resets to Slow Start after packet loss detection. This can lead to underutilization of available bandwidth. More efficient in handling packet loss scenarios as it maintains a higher cwnd during Fast Recovery, allowing for better utilization of network resources. <p>![[Pasted image 20240921122340.png]]</p>"},{"location":"notes/Transport%20layer/#additive-increase-multiplicative-decrease-aimd","title":"Additive Increase Multiplicative Decrease (AIMD)","text":"<p>AIMD is a fundamental principle behind TCP congestion control. It combines additive increase (gradually increasing cwnd) with multiplicative decrease (halving cwnd upon detecting congestion). This balance allows TCP to efficiently utilize network resources while responding appropriately to congestion signals.</p>"},{"location":"notes/Transport%20layer/#macroscopic-description-of-tcp-throughput","title":"Macroscopic Description of TCP Throughput","text":"<p>The throughput of TCP can be described on a macroscopic level as being influenced by factors such as round-trip time (RTT), packet size, and network conditions. The throughput tends to stabilize around a certain value determined by these factors, demonstrating how effectively TCP can adapt to varying network environments.</p> <p>w - window </p> <p>![[Pasted image 20240921124720.png]]</p>"},{"location":"notes/Transport%20layer/#tcp-over-high-bandwidth-paths","title":"TCP Over High-Bandwidth Paths","text":"<p>When operating over high-bandwidth paths, traditional TCP may not fully utilize available bandwidth due to its conservative nature in adjusting cwnd. Techniques such as increasing the initial cwnd size or implementing larger MSS can help improve performance on high-speed networks.</p>"},{"location":"notes/Transport%20layer/#fairness","title":"Fairness","text":"<p>Fairness in network protocols refers to how bandwidth is allocated among multiple users or connections. In the context of TCP, fairness ensures that all connections receive an equitable share of available bandwidth without one connection monopolizing resources.</p>"},{"location":"notes/Transport%20layer/#fairness-and-udp","title":"Fairness and UDP","text":"<p>Unlike TCP, which implements congestion control mechanisms, User Datagram Protocol (UDP) does not have built-in mechanisms for fairness or congestion management. This can lead to scenarios where UDP traffic can dominate available bandwidth without regard for other traffic types.</p>"},{"location":"notes/Transport%20layer/#fairness-and-parallel-tcp-connections","title":"Fairness and Parallel TCP Connections","text":"<p>When multiple parallel TCP connections are established between two endpoints, they must compete for shared resources. Properly designed algorithms ensure that these connections fairly share bandwidth while minimizing congestion effects on overall performance.</p>"},{"location":"notes/Transport%20layer/#explicit-congestion-notification-ecn","title":"Explicit Congestion Notification (ECN)","text":"<p>ECN is an advanced mechanism that allows routers to signal impending congestion to endpoints before packet loss occurs. By marking packets instead of dropping them, ECN enables senders to reduce their transmission rates proactively, enhancing overall network stability.</p>"},{"location":"notes/Transport%20layer/#network-assisted-congestion-control","title":"Network-assisted Congestion Control","text":"<p>Network-assisted congestion control involves utilizing feedback from network devices (like routers) to inform endpoints about current network conditions. This feedback can help adjust transmission rates more effectively than traditional end-to-end methods alone, leading to better utilization of network resources and reduced congestion.</p> <p>These principles collectively form a comprehensive framework for managing data flow in networks using TCP, ensuring efficient communication while minimizing potential issues related to congestion.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/","title":"Upper Layers in Detail","text":"<p>Note that the things written here about the OSI model are also applicable in the TCP/IP model.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#application-layer","title":"Application Layer","text":"<p>The application layer serves as the interface or portal for applications to communicate over networks. It provides services such as email, file transfer, and web browsing.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#presentation-layer","title":"Presentation Layer","text":"<p>The presentation layer is responsible for making data presentable by handling data formats and encryption. It converts data into standardized formats such as PDF or HTML and ensures data security through encryption protocols like SSL.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#session-layer","title":"Session Layer","text":"<p>The session layer facilitates communication between the source and destination while other layers perform their functions. It manages sessions between applications and provides services such as establishing, maintaining, and terminating connections.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#transport-layer","title":"Transport Layer","text":"<p>The transport layer determines how data is delivered from the source to the destination. It selects appropriate protocols, such as TCP for reliable transmission or UDP for faster transmission. TCP ensures reliable delivery by waiting for acknowledgment, while UDP prioritizes speed over reliability.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#tcp-vs-udp","title":"TCP vs UDP","text":"<ul> <li>TCP: Reliable transmission, waits for acknowledgment.</li> <li>UDP: Faster transmission, does not wait for acknowledgment.</li> </ul> <p>Example: YouTube uses TCP for sending web page data and switches to UDP for streaming videos to prioritize real-time delivery.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#ports","title":"Ports","text":"<p>Ports are used at the transport layer to facilitate communication between devices. They allow the destination to provide different services on different ports.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#purpose-of-ports","title":"Purpose of Ports","text":"<p>Ports enable multiple services to run on a single device by assigning unique port numbers to each service. For example, HTTPS traffic typically uses port 443, FTP uses port 21, and SSH uses port 22.</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#source-port-assignment","title":"Source Port Assignment","text":"<p>When a device communicates with another device, it is assigned a source port to allow the destination to communicate back. This assigned port, known as an ephemeral port, is chosen from the range of available ports (0-65,535).</p>"},{"location":"notes/Upper%20Layers%20in%20Detail/#ephemeral-port","title":"Ephemeral Port","text":"<ul> <li>An ephemeral port is a randomly assigned port used by the sender for communication.</li> <li>It allows the destination to reply to the sender's communication.</li> </ul> <p>Ports with numbers\u00a0<code>0-1023</code>\u00a0are called system or\u00a0well-known\u00a0ports; ports with numbers\u00a0<code>1024-49151</code>\u00a0are called user or\u00a0registered\u00a0ports, and ports with numbers\u00a0<code>49152-</code>65535`\u00a0are called dynamic, private or\u00a0ephemeral\u00a0ports. Registered port numbers are currently assigned by the... IANA... and were assigned by... ICANN... before March 21, 2001, and were assigned by the... USC/ISI... before 1998.</p> <p>Example Communication: <code>your_ip_addr:ephemeral_port --&gt; known_ip_addr:443</code> (443 for HTTPS)</p> <p>Read more about ephemeral ports here.</p>"},{"location":"notes/control%20plane/","title":"Control plane","text":""},{"location":"notes/control%20plane/#52-routing-algorithms","title":"5.2 Routing algorithms","text":"<ul> <li>goal of a routing algo is to provide a good path that provides the least cost.<ul> <li>good: least cost, fastest path, least congested path</li> </ul> </li> <li>Graph abstraction<ul> <li>Graph G = (N, E)</li> <li>N: set of routers</li> <li>E: set of links</li> <li>Each router that is directly connected may have a set cost, but routers that aren't directly connected have a cost of infinity. </li> <li>Cost maybe 1 or defined by router operator or based on other conditions (congestion, bandwidth etc)</li> </ul> </li> </ul> <p>![[Pasted image 20241125121216.png]]</p>"},{"location":"notes/control%20plane/#link-state-algorithm","title":"Link State Algorithm","text":"<ul> <li>all link costs are already known and act as an input to the LS algorithm</li> <li>a type of LS algo is Dijkstra algo</li> <li>this computes the least-cost path from one node to all other nodes in the network</li> <li>kth iteration -&gt; least cost paths of K nodes</li> </ul> <p>![[Pasted image 20241125152320.png]] ![[Pasted image 20241125155615.png]]</p>"},{"location":"notes/control%20plane/#oscillations","title":"oscillations","text":""},{"location":"notes/control%20plane/#distance-vector-algorithm","title":"distance vector algorithm","text":"<ul> <li>decentralized, iterative, asynchronous and iterative type of algorithm<ul> <li>decentralized: doesn't have information of all the nodes, only info of the directly connected neighbors</li> <li>distributed: each node receives some info from its neighbours</li> <li>async: all nodes need to operate in lockstep with each other</li> <li>iterative: process continues until no more information is exchanged between neighbours.</li> <li>$min_v$ in the equation is taken over all of x\u2019s neighbors</li> </ul> </li> <li>in this algo each node x consists of <ul> <li>c(x,v): cost for x -&gt; v</li> <li>D_x (distance vector of x): x's estimate of its cost to its neighbors</li> <li>D_v (distance vector of neighbors)</li> </ul> </li> <li>least cost for a node is calculated using the Bellman ford equation for a node![[Pasted image 20241125180759.png]]</li> </ul> <p>cost of \"me\" to \"my neighbor\" + cost of \"my neighbor\" to destination</p> <ul> <li>Example of a topology![[Pasted image 20241125180849.png]]</li> <li>New computations for node b using BF equation![[Pasted image 20241125182152.png]]</li> <li>DV-like algorithms are used in many routing protocols in practice, including the Internet\u2019s RIP and BGP, ISO IDRP, Novell IPX, and the original ARPAnet.</li> <li>\"Good news travels fast\" &amp; \"Bad news travels slow\"<ul> <li>Good news: a decrease in the cost of one of the links</li> <li>Bad news: an increase in the cost of one of the links</li> <li>The increase in cost can create a routing loop as the DV algo doesn't update the nodes properly.</li> <li>In the (b) diagram, the change from 4 -&gt; 60 would take a lot of cycles until z knows that y -&gt; x cost is 60. Whereas the decrease in cost, 4 -&gt; 1, takes 3 cycles (t0, t1, t2) ![[Pasted image 20241125190011.png]]</li> </ul> </li> </ul> <p>![[Pasted image 20241125190701.png]]</p> <p>Black holing in distance vector algorithms, particularly in the context of routing protocols like AODV (Ad Hoc On-Demand Distance Vector), refers to a type of denial-of-service (DoS) attack where a malicious node falsely claims to have a valid route to a destination. This node then drops all packets sent through it, effectively \"black holing\" the data and preventing it from reaching its intended destination.</p>"},{"location":"notes/control%20plane/#bellman-ford-example","title":"bellman ford example","text":"<p>all tables initially![[Pasted image 20241015105045.png]]</p> <p>updating table N1![[Pasted image 20241015105540.png]]</p> <p>updating table n5![[Pasted image 20241015111056.png]] ![[Pasted image 20241015111235.png]]</p> <p>updating table n2![[Pasted image 20241015113301.png]]</p>"},{"location":"notes/control%20plane/#53-intra-as-routing-in-the-internet-ospf","title":"5.3 intra-as routing in the internet: OSPF","text":"<p>All routers being homogenous (in terms of their routing algo) is simplistic for two reasons - Scale     - the global network has billions of devices, each device couldn't possibly contain a record for all billion devices.     - the complexity of making the algo work would be extremely hard. - Autonomy     - each ISP should be allowed to use their own algorithms within their network     - and at the same time connect with the outside network</p> <p>These problems can be solved using Autonomous systems, a group of routers under the same administrative control.</p> <ul> <li>Intra AS: routing among the same network/AS<ul> <li>gateway router at the edge of an AS helps with the connection of different AS'es.</li> <li>common protocols<ul> <li>RIP (DV algo, now used for Inter)</li> <li>OSPF (link state algo)</li> <li>EIGRP (DV algo)</li> </ul> </li> </ul> </li> <li>Inter AS: routing among different AS'es<ul> <li>gateways perform inter AS routing</li> <li>common protocols<ul> <li>Border Gateway Protocol (BGP)</li> </ul> </li> </ul> </li> </ul>"},{"location":"notes/control%20plane/#open-shortest-path-first","title":"Open Shortest Path First","text":"<ul> <li>\"open\": specifications are publicly available.</li> <li>link state protocol.</li> <li>link-state advertisement packets are flooded to other routers in the AS.</li> <li>the state is broadcasted every 30 minutes.</li> <li>the advertisement is sent along with the OSPF message which is transferred by the IP where the header for upper layer protocol is set to port 89.</li> <li>each router locally runs the Dijkstra algorithm to determine the shortest path tree to all subnets</li> <li>Provides security<ul> <li>Simple<ul> <li>configured password is attached to the OSPF packet in plain text</li> <li>not completely secure</li> </ul> </li> <li>MD5<ul> <li>hash(preconfig password + OSPF content)</li> <li>hash is added to OSPF packet</li> <li>destination does a hash(packet) and compare w/ hash value that the packet carries.</li> </ul> </li> </ul> </li> <li>Allows multiple paths to be taken if they have same costs.</li> <li>Integrated support for multicast (MOSPF - multicast OSPF) Zand anycast routing.</li> <li>Supports hierarchy within an AS.<ul> <li>example of two level hierarchy<ul> <li>consists of multiple areas. each area has its own border router.</li> <li>these areas are connected via a backbone. the border router is part of this backbone.</li> <li>areas have info only about their area, communicate w/ other areas through backbone.</li> <li>in the backbone, there is a boundary router which helps in connecting with other AS'es.</li> </ul> </li> </ul> </li> <li>Example of hierarchical OSPF ![[Pasted image 20241125220113.png]]</li> </ul>"}]}